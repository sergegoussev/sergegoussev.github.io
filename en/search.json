[
  {
    "objectID": "content/blogs/tech-topics/itables/index.html",
    "href": "content/blogs/tech-topics/itables/index.html",
    "title": "Explore python itables library to render tables",
    "section": "",
    "text": "As much of the data I use is tabular - I often have to visualize the dataframe, pivot it, filter it, and then visualize it again. Since I may want to go back to the dataset later - making sure I can repeat this mini-EDA again - is quite useful. Downloading an excel for manual exploration is obviously an option but not an ideal one.\nPython has an itables library that helps render dataframes as pretty tables. These can also be integrated with quarto to render into the html.\nThis blog summarizes several key features and how to turn them on.\n\nimport pandas as pd\n\nfrom itables import init_notebook_mode\nfrom itables import show\n\nimport itables.options as opt\n\n#raise the size of the data made availible othewize medium (or even some small) dataframes will be downsampled.\nopt.maxBytes = 131072\nopt.maxColumns = 0\n\ninit_notebook_mode(all_interactive=True)\n\nopt.ordering = False \n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#purpose-of-the-blog",
    "href": "content/blogs/tech-topics/itables/index.html#purpose-of-the-blog",
    "title": "Explore python itables library to render tables",
    "section": "",
    "text": "As much of the data I use is tabular - I often have to visualize the dataframe, pivot it, filter it, and then visualize it again. Since I may want to go back to the dataset later - making sure I can repeat this mini-EDA again - is quite useful. Downloading an excel for manual exploration is obviously an option but not an ideal one.\nPython has an itables library that helps render dataframes as pretty tables. These can also be integrated with quarto to render into the html.\nThis blog summarizes several key features and how to turn them on.\n\nimport pandas as pd\n\nfrom itables import init_notebook_mode\nfrom itables import show\n\nimport itables.options as opt\n\n#raise the size of the data made availible othewize medium (or even some small) dataframes will be downsampled.\nopt.maxBytes = 131072\nopt.maxColumns = 0\n\ninit_notebook_mode(all_interactive=True)\n\nopt.ordering = False \n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#example-data",
    "href": "content/blogs/tech-topics/itables/index.html#example-data",
    "title": "Explore python itables library to render tables",
    "section": "Example data",
    "text": "Example data\nLets generate a complex set of data that has a diverse set of columns\n\ndata_for_df = {\n    'product_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115],\n    'price': [12.50, 19.99, 5.00, 25.75, 45.20, 10.99, 8.50, 30.00, 9.99, 15.25, 20.00, 7.50, 22.40, 18.99, 35.10],\n    'category': ['Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries'],\n    'customer_rating': [4, 5, 3, 5, 4, 5, 4, 3, 5, 4, 5, 3, 4, 5, 4],\n    'manufacturer': [\n        '&lt;p&gt;&lt;b&gt;Advanced Technology Solutions, a global leader&lt;/b&gt;&lt;p&gt;',\n        'Independent Authors Collective, specializing in unique fiction',\n        'Small-Town Organic Farmers Cooperative',\n        'Global Tech Innovations Group',\n        'The Classic Book Reprint Company, preserving literary heritage',\n        'Fresh Harvest Supply Chain and Distribution',\n        'Audio Excellence, an innovative headphones manufacturer',\n        'The Aspiring Coder\\'s Press and Digital Media',\n        'Sustainable Coffee Roasters Association',\n        'Smart Living Devices and Automation Solutions',\n        'Renowned Historical Accounts Publishing House',\n        'Local Produce Delivery for the Modern Consumer',\n        'Elite Computing Innovations, a leader in high-performance hardware',\n        'Emerging Voices Poetry Guild',\n        'Professional Imaging and Optics Corporation'\n    ],\n    'region': ['North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia'],\n    'long_description': [\n        'This is a long description for the first product, highlighting its premium features and durability.',\n        'An enthralling novel with a captivating plot and unforgettable characters that will keep you on the edge of your seat.',\n        'Freshly sourced organic apples, perfect for a healthy snack or for baking delicious homemade pies.',\n        'A high-performance gadget with advanced technology and a sleek, modern design that is easy to use and carry.',\n        'A classic mystery story with an unexpected twist, a great read for all fans of the genre.',\n        'A selection of artisanal breads, baked fresh daily with high-quality ingredients and a crispy crust.',\n        'Innovative noise-canceling headphones providing crystal-clear audio and a comfortable fit for extended listening sessions.',\n        'A comprehensive guide to coding best practices, offering practical examples and detailed explanations for beginners.',\n        'Gourmet coffee beans sourced from sustainable farms, roasted to perfection for a rich and smooth flavor.',\n        'A versatile smart home device that can control various appliances and is compatible with most other systems.',\n        'A riveting biography of a historical figure, full of intriguing details and little-known anecdotes from their life.',\n        'Hand-picked seasonal vegetables delivered fresh to your door, perfect for preparing a wide variety of meals.',\n        'An ultra-thin tablet with a high-resolution display, making it ideal for both entertainment and professional use.',\n        'A timeless poetry collection featuring works from both well-known and up-and-coming authors.',\n        'A powerful and compact camera perfect for capturing high-quality photos and videos on the go.'\n    ]\n}\n\ndf = pd.DataFrame.from_dict(data_for_df)\n\nNow lets see some ways this can be worked with"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#default-visual",
    "href": "content/blogs/tech-topics/itables/index.html#default-visual",
    "title": "Explore python itables library to render tables",
    "section": "Default visual",
    "text": "Default visual\n\nshow(df)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#advanced-version",
    "href": "content/blogs/tech-topics/itables/index.html#advanced-version",
    "title": "Explore python itables library to render tables",
    "section": "Advanced version",
    "text": "Advanced version\nLets make some changes so that we can modify column widths, add word-wrap, and change defaults for specific columns\n\nshow(\n    df,\n    # Disable auto-width and use a fixed layout for reliable column width.\n    autoWidth=False,\n    style=\"table-layout:fixed\",#; width:800px;\",\n    # Add buttons to customie actions that can be taken\n    buttons=['pageLength', 'csvHtml15','colvis'],\n    # Add a search pagen to ensure that I can filter the data in some way\n    layout={\"top1\":'searchPanes'},\n    # Modify columns\n    columnDefs=[\n        # Modify width of overly wide columns and make them word wrap\n        {\"width\": \"200px\", \"targets\": [4, 6], \"className\": \"word-wrap\"},\n        # Hide some columns by default\n        {\"targets\": [2,3], \"visible\": False},\n    ],\n    scrollX=False,\n)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/datasets/polish-scanner/index.html",
    "href": "content/blogs/datasets/polish-scanner/index.html",
    "title": "Polish scanner dataset",
    "section": "",
    "text": "Jacek Bialek recently uploaded this scanner dataset of polish data to zenodo. This blog is my personal exploration of this dataset.\nThis is the init_notebook_mode cell from ITables v2.6.2\n(you should not see this message - is your notebook trusted?)"
  },
  {
    "objectID": "content/blogs/datasets/polish-scanner/index.html#basic-dataset-overview",
    "href": "content/blogs/datasets/polish-scanner/index.html#basic-dataset-overview",
    "title": "Polish scanner dataset",
    "section": "Basic dataset overview",
    "text": "Basic dataset overview\nFirst off, let‚Äôs look at the raw dataset\n\n\nShow the code\ndf = pd.read_csv(\"https://zenodo.org/records/18342253/files/dataRSM.csv?download=1\", sep=\";\", index_col=False)\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8090 entries, 0 to 8089\nData columns (total 9 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   time           8090 non-null   object\n 1   prices         8090 non-null   object\n 2   quantities     8090 non-null   int64 \n 3   retID          8090 non-null   object\n 4   description    8090 non-null   object\n 5   retailer_code  8090 non-null   int64 \n 6   EAN_code       8090 non-null   object\n 7   category       8090 non-null   object\n 8   subcategory    8090 non-null   object\ndtypes: int64(2), object(7)\nmemory usage: 569.0+ KB\n\n\nRendering it in its raw format isn‚Äôt very useful as prices and time (for example), are objects. Hence some light processing would be in order.\n\n\nShow the code\nmy_dtypes = {\n    'prices': 'float64', \n    'description': 'str'\n}\n\ndf = pd.read_csv(\n    \"https://zenodo.org/records/18342253/files/dataRSM.csv?download=1\", \n    sep=\";\", \n    decimal=\",\",\n    dtype=my_dtypes,\n    parse_dates=['time'],\n    index_col=False\n)\n\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8090 entries, 0 to 8089\nData columns (total 9 columns):\n #   Column         Non-Null Count  Dtype         \n---  ------         --------------  -----         \n 0   time           8090 non-null   datetime64[ns]\n 1   prices         8090 non-null   float64       \n 2   quantities     8090 non-null   int64         \n 3   retID          8090 non-null   object        \n 4   description    8090 non-null   object        \n 5   retailer_code  8090 non-null   int64         \n 6   EAN_code       8090 non-null   float64       \n 7   category       8090 non-null   object        \n 8   subcategory    8090 non-null   object        \ndtypes: datetime64[ns](1), float64(2), int64(2), object(4)\nmemory usage: 569.0+ KB"
  },
  {
    "objectID": "content/blogs/datasets/polish-scanner/index.html#exploring-the-dataset",
    "href": "content/blogs/datasets/polish-scanner/index.html#exploring-the-dataset",
    "title": "Polish scanner dataset",
    "section": "Exploring the dataset",
    "text": "Exploring the dataset\nUsing the itables library, have a way to play with the dataset\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.6.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/datasets/polish-scanner/index.html#trends",
    "href": "content/blogs/datasets/polish-scanner/index.html#trends",
    "title": "Polish scanner dataset",
    "section": "Trends",
    "text": "Trends\n\nNumbers of time\nFirst off, let‚Äôs see how many unique rows (i.e.¬†number of transaction observations) were observed over time\n\n\nShow the code\nfig = px.histogram(df, \n                   x=\"time\", \n                   color=\"category\", \n                   title=\"Number of observations over time (by category)\",\n                   barmode='stack')\nfig.update_layout(bargap=0.3)\nfig.show()\n\n\n                            \n                                            \n\n\nWe can also look at the number of unique products over time (focusing on EANs, common to European stores)\n\n\nShow the code\ndf_unique = (df.groupby([pd.Grouper(key='time', freq='ME'), 'category'])\n             ['EAN_code']\n             .nunique()\n             .reset_index(name='unique EANs'))\n\nfig = px.bar(df_unique, \n             x=\"time\", \n             y=\"unique EANs\", \n             color=\"category\",\n             title=\"Unique Products (i.e. EANs) across timee\",\n             barmode='stack')\n\nfig.update_layout(bargap=0.4)\n\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nPrice trends\nWe can also look at price trends by category\n\n\nShow the code\ndf_avg = (df.groupby([pd.Grouper(key='time', freq='ME'), 'category'])['prices']\n            .mean()\n            .reset_index(name='price'))\n\nfig = px.line(df_avg,\n              x=\"time\",\n              y=\"price\",\n              facet_col=\"category\",\n              facet_col_wrap=2,\n              title=\"Average Price by Category\",\n              markers=True)\n\nfig.show()\n\n\n                            \n                                            \n\n\nAs well as by subcategory\n\n\nShow the code\ndf_avg = (df.groupby([pd.Grouper(key='time', freq='ME'), 'subcategory'])['prices']\n            .mean()\n            .reset_index(name='price'))\n\nfig = px.line(df_avg,\n              x=\"time\",\n              y=\"price\",\n              facet_col=\"subcategory\",\n              facet_col_wrap=4,\n              title=\"Average Price by Subcategory\",\n              markers=True)\n\nfig.show()"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html",
    "href": "content/blogs/datasets/italian-ws/index.html",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "",
    "text": "Daniele Sasso and a few others made their dataset availible on Zenodo - https://doi.org/10.5281/zenodo.14927602 - daily webscraped data from different shops of an Italian supermarket chain. This blog summarizes the dataset and explores its various facets. Detailed overview of the data is available on the Price Stats Catalogue record of this dataset and some explorations below are summarized there."
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#dataset-structure",
    "href": "content/blogs/datasets/italian-ws/index.html#dataset-structure",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Dataset structure",
    "text": "Dataset structure\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#general-overview-of-the-dataset",
    "href": "content/blogs/datasets/italian-ws/index.html#general-overview-of-the-dataset",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "General overview of the dataset",
    "text": "General overview of the dataset\nFirst off - lets look at the data itself, its columns, and some statistics about the web scraping itself.\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4033211 entries, 0 to 4033210\nData columns (total 8 columns):\n #   Column      Dtype  \n---  ------      -----  \n 0   date        object \n 1   price       float64\n 2   product_id  int64  \n 3   store_id    int64  \n 4   region      object \n 5   product     object \n 6   COICOP5     object \n 7   COICOP4     object \ndtypes: float64(1), int64(2), object(5)\nmemory usage: 246.2+ MB"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#detailed-info-about-the-dataset",
    "href": "content/blogs/datasets/italian-ws/index.html#detailed-info-about-the-dataset",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Detailed info about the dataset",
    "text": "Detailed info about the dataset\nAs this is web scrape data for several years - its saved all in one analytical table.\nLet‚Äôs look at it in a bit more detail:\n\n\nShow the code\nstats = {}\nstats['Number of unique products'] = df['product'].nunique()\nstats['Number of unique stores'] = df['store_id'].nunique()\nstats['Number of unique regions'] = df['region'].nunique()\nstats['Number of COICOP5 categories'] = df['COICOP5'].nunique()\nstats['Number of unique scrapes'] = df['date'].nunique()\nstats['Number of average unique products per store per date'] = round(df.groupby([\"date\", \"store_id\"])[\"product_id\"].nunique().reset_index()['product_id'].mean(),1)\nd_end = datetime.fromisoformat(df['date'].max())\nd_start = datetime.fromisoformat(df['date'].min())\nd = d_end-d_start\nstats['number of days in sample'] = d.days + 1\npd.DataFrame.from_dict(stats, orient='index', columns=['statistic'])\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nIt seems that there are 863 days but 841 scrapes - that means that there were no scrapes during 22 days:\n\n\nShow the code\n# Compare the current scrape list\nscrape_dates = pd.DatetimeIndex(df['date'].unique())\n\n# Against an uninterupted list of dates\nstart_date = scrape_dates.min()\nend_date = scrape_dates.max()\n\n# Create a complete, continuous date range\nfull_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n# Find the dates that are in the full range but not in the scrape list\nmissing_dates = full_date_range.difference(scrape_dates)\n# missing_dates\n\nmissing_dates\n\n\nDatetimeIndex(['2021-02-13', '2021-02-14', '2021-03-23', '2021-03-28',\n               '2021-06-24', '2021-08-22', '2021-08-23', '2021-08-24',\n               '2021-08-25', '2021-08-26', '2021-08-27', '2021-09-30',\n               '2021-12-02', '2022-01-23', '2022-03-16', '2022-06-21',\n               '2022-10-01', '2022-10-07', '2022-10-10', '2022-10-22',\n               '2022-10-23', '2022-11-04'],\n              dtype='datetime64[ns]', freq=None)\n\n\nIf we pivot the raw data and show the number of prices captured per store per region - it looks like this:\n\n\nShow the code\ndf.pivot_table(index='date', columns=['region','store_id'], aggfunc='count')\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nLets also look at the number of stores per region (i.e.¬†the above but visually)\n\n\nShow the code\ndf_number_of_stores_per_region = df.groupby([\"region\"])[\"store_id\"].nunique().to_frame()\ndf_number_of_stores_per_region.plot(kind='bar', color='green', figsize=(10,4))\nplt.title('Number of stores per region')\nplt.xlabel('region')\nplt.ylabel('Number of stores')\n# plt.xticks(rotation=1)  # Keep x-axis labels horizontal\nplt.show()"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#trends-about-what-was-captured",
    "href": "content/blogs/datasets/italian-ws/index.html#trends-about-what-was-captured",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Trends about what was captured",
    "text": "Trends about what was captured\n\nBy category\nLets look at the number of unique products and the number of web offers captured by COICOP5 category\n\n\nShow the code\ndf_coicop_categories = df.groupby([\"COICOP5\"])[\"product_id\"].nunique()\nmean = df_coicop_categories.mean()\n\nfix, ax = plt.subplots()\n\ndf_coicop_categories.plot(\n    kind=\"bar\",\n    figsize=(12,4),\n    title=\"Number of unique products per COICOP5 category\",\n    color='darkblue',\n    legend=False,\n    ax=ax\n)\n\nax.axhline(mean, color='red', alpha=0.5)\n\nax.text(\n    x=19,\n    y=mean,\n    s=f'Average: {round(mean,1)}',\n    color='red',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndf_coicop_categories = df.groupby([\"COICOP5\"])[\"product_id\"].count()\nmean = df_coicop_categories.mean()\n\nfix, ax = plt.subplots()\n\ndf_coicop_categories.plot(\n    kind=\"bar\",\n    figsize=(12,4),\n    title=\"Number of web offers per COICOP5 category\",\n    color=\"#24dba4\",\n    legend=False,\n    ax=ax\n)\n\nax.axhline(mean, color='red', alpha=0.5)\n\nax.text(\n    x=19,\n    y=mean,\n    s=f'Average: {round(mean,1)}',\n    color='red',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nOver time\nWe can also consider how much data was captured across time\n\n\nShow the code\ndf2 = df.copy(deep=True)\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2 = df2.set_index('date')\ndf_scrapes = df2.resample('ME')['product_id'].count().to_frame()\nscrape_mean_monthly = df_scrapes['product_id'].mean()\n\nfix, ax = plt.subplots()\n\ndf_scrapes.plot(\n    kind='bar',\n    figsize=(12,4),\n    title=\"Number of web offers scraped per month, along with an average\",\n    legend=False,\n    ax=ax)\nplt.ylabel(\"number of web offers\")\n\nax.axhline(scrape_mean_monthly, color='red', alpha=0.5, label=\"average\")\n\nax.text(\n    x=20,\n    y=scrape_mean_monthly,\n    s=f'Average: {round(scrape_mean_monthly,1)}',\n    color='red',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nIt seems that the amount of web offers started to decline. This should probably be investigated (if its region or store coverage) to see if longitudinal time series should exclude any of this data"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#geographic-distribution-of-unique-products-by-region",
    "href": "content/blogs/datasets/italian-ws/index.html#geographic-distribution-of-unique-products-by-region",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Geographic distribution of unique products by region",
    "text": "Geographic distribution of unique products by region\nThere is some example code in the zenodo page for the dataset that shows well some of the price/product info captured\n\n\nShow the code\n# Convert date column\ndf['date'] = pd.to_datetime(df['date'])  # Format: YYYY-MM-DD\n\n# Define category colors\ncategory_colors = {\"Fruit\": \"blue\", \"Vegetable\": \"green\", \"Meat\": \"red\"}\n\ngeo = df.groupby([\"region\", \"COICOP4\"])[\"product_id\"].nunique().reset_index()\npivot_geo = geo.pivot(index=\"region\", columns=\"COICOP4\", values=\"product_id\").fillna(0)\npivot_geo[\"Total\"] = pivot_geo.sum(axis=1)\npivot_geo = pivot_geo.sort_values(\"Total\", ascending=False).drop(columns=\"Total\")\npivot_geo = pivot_geo[[\"Fruit\", \"Meat\", \"Vegetable\"]]\n\npivot_geo.plot(kind=\"bar\", stacked=True, figsize=(10,6), color=[\"blue\", \"red\", \"green\"])\nplt.ylabel(\"Number of Unique Products\")\nplt.title(\"Geographic Distribution by Region and Category (Sorted)\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Category\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#basic-analysis-average-price-trend-over-time-by-coicop4",
    "href": "content/blogs/datasets/italian-ws/index.html#basic-analysis-average-price-trend-over-time-by-coicop4",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Basic analysis: average price trend over time (by COICOP4)",
    "text": "Basic analysis: average price trend over time (by COICOP4)\nWe can also look at average prices by COICOP4 over time\n\n\nShow the code\nprice_trend = df.groupby([\"date\", \"COICOP4\"])[\"price\"].mean().reset_index()\n\nplt.figure(figsize=(10,5))\nsns.lineplot(data=price_trend, x=\"date\", y=\"price\", hue=\"COICOP4\", palette=category_colors)\nplt.title(\"Average Price Over Time by COICOP4 Category\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Average Price (‚Ç¨)\")\nplt.legend(title=\"Category\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html",
    "title": "Sampling methods for scanner data",
    "section": "",
    "text": "NoteEmpirical example TBC\n\n\n\n\n\nThe blog currently summaries the methodological piece, however the empricial example using public scanner data is a WIP."
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html#the-concept",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html#the-concept",
    "title": "Sampling methods for scanner data",
    "section": "The concept",
    "text": "The concept\n\nThe fixed sample method simulates a traditional approach, similar to field collection. The sample is refreshed every year to stay representative and replacement is used to maintain the sample following a traditional methodology (except that scanner data is available to sample from, i.e.¬†you can seeing all products and all transactions, which provides a lot of additional detail of what product to replace with).\nThe method is simple (in the sense that it follows a traditional methodology and can be combined with other data in a traditional way), however it is resource intensive as replacement must be dealt with manually (for example Netherlands found it too costly to extend to 6 scanner retailers and shifted to the dynamic sample method (Grient and Haan 2010)). It is also typically used with only a small (although representative) sample of data and can be the first approach used when starting out with scanner data.\nThe fixed sample method favors comparability over representativity as the same items are compared over time while the representativity of the sample deteriorates over time (which is only partially mitigated by annual product resampling):\n\nA consequence of the method is thus that only similar products are selected for most disappearing items (to avoid quality adjustment), which means the method is most applicable to situations where quality change is not a major factor in the category.\nThe method is also only applicable to categories with little churn as representativity of the products in sample will decline quickly if this is not the case. Its best to not use this method too widely due to lower representativity."
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html#step-by-step-process",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html#step-by-step-process",
    "title": "Sampling methods for scanner data",
    "section": "Step-by-step process",
    "text": "Step-by-step process\nThe summary of the overall process is best summarized by (Eurostat 2017) (specifically chapter 6):\n\nIn the static approach, a sample is drawn from year t and used for 12 months following December of year t. The sample is kept, and replacements are made as needed.\n\nIn a more detailed sense, the following steps are used to select and maintain the sample:\n\nStep 1: For each category (typically COICOP 6) for which an elementary price index will be calculated, select a number of items (representative products) at the beginning of the year which were representative the previous year. For instance, products that constituted the top 50% of turnover (i.e.¬†using cut-off sampling) for the reference month, several months, or whole year can be included:\n\nSelect unique products (such as by European Article Number (EAN)) per supermarket chain and stores (to ensure that coverage by all chains in the sample is maintained)\nWith this approach, we may end up with a lot of observations per category, or too few, in which case a little tuning is necessary.\nEach item given a weight representing its relative importance.\n\nStep 2: A monthly price index (i.e.¬†relative) for each item is calculated as the unit value in the current month and unit value in the base year.\nStep 3: An elementary price index is calculated using item relatives in the traditional way. Two ways can be used:\n\nA Jevons approach, the same as field collection data for elementary aggregates where price relatives are calculated at the representative product by region level.\nA weighted Laspeyres approach using each item‚Äôs relatives as input, which is akin to higher level aggregation.\n\nOther considerations:\n\nFor the Laspeyres type approach, chain the short-term indices on the December month to create a long-term series.\nOutlier methods as well as a dumping filter should be implemented to detect and remove unusual prices.\nImputation can be used for one or two months before a replacement is selected.\nIf an item is considered permanently missing or it is found to not be representative anymore, a new item needs to be selected that is not yet in the basket but was present in the base period. For instance, if there is a big shift (but no exit), a product could be replaced manually. Quality adjustment may be necessary (see CPI Manual, chapter 7 for more detail on how to maintain methods):\n\nExplicit quality adjustment used only when needed in a select number of cases (such as if there is a change in the content of an item, quantity adjustment can be used).\nImplicit methods (typically the overlap method, but also mean imputation) may also need to be used."
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html#further-reading",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html#further-reading",
    "title": "Sampling methods for scanner data",
    "section": "Further reading",
    "text": "Further reading\nFor more info, check out Larsen (2014), Grient and Haan (2010), and Lamboray (2024)."
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html#the-concept-1",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html#the-concept-1",
    "title": "Sampling methods for scanner data",
    "section": "The concept",
    "text": "The concept\n\nAs the distribution of expenditures is typically quite skewed (Antoniades, Xu, and Feenstra 2017), a relatively small number of items are usually representing the majority of expenditures. The method focuses on these using a cut-off-sampling approach and allows the basket (sample) to be updated (i.e.¬†re-sampled) every month. Put differently, ‚Äúan item will be used in the computation of the index between two consecutive months if its average expenditure share (with respect to the set of matched items) in those months is above a certain threshold value‚Äù (Grient and Haan 2010).\nAn unweighted (i.e.¬†not using turnover) Jevons is used and chained, thereby reducing the risk of chain drift (as weights are used implicitly for sampling of items but not explicitly for index calculation).\nThe other added benefit is that it shifts to an automatic way to maintain the sample, as the process to maintain the fixed sample method is also too labor intensive and hard to extend to many retailers (6 in the Netherlands case).\nThe dynamic method was also found to closely approximate the GEKS, hence its a useful way to measure food and non-food categories. After a while, the NSO can shift to the multilateral method (requires a minimum of 13 months, ideally 25).\nImplementation requires choosing appropriate dumping and outlier filters prior to implementing."
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html#step-by-step-process-1",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html#step-by-step-process-1",
    "title": "Sampling methods for scanner data",
    "section": "Step-by-step process",
    "text": "Step-by-step process\n\nStep 1: Calculate movement (price relative) for each product in the category based on the unit value price in this month over unit value price the previous month.\n\nPre-processing is needed to calculate unit prices.\nFiltering/cleaning is also applied:\n\nFor instance, an outlier filter could exclude anything 300% higher and 75% decline.\nA dumping filter could be used to exclude strong simultaneous price and quantity decreases ‚Äì as this dumping in prices without an offsetting price increase will have a downward effect on the index.\n\nThe applicable unique identifier (EAN in the Dutch example) is used for matched-model method, but cleaning/investigation needed to make sure identifier is stable (sometimes retailers recycle EANs for instance). SKU can be used where applicable as it may be more stable (sometimes capture the relaunch problem). If the identifier is too detailed for CPI purposes (say where two products with different codes are identical from a consumers‚Äô point of view), these should be grouped in theory but in principle can be explicitly quality adjusted.\nOnly the first several weeks of data are used ‚Äì the Dutch case its 3 weeks, consistent with field collection.\n\nStep 2: Use a cut-off method to select products that represent a most of the sales in the category ‚Äì general, a relatively small proportion of products will be responsible for the majority of expenditures and would carry the most weight in weighted indices (hence the cut-off method prioritizes the same products). The threshold chosen in the Dutch case was that 50% of the items are selected to represent 80-85% of expenditures\n\n¬†¬†¬†¬†¬†\\(\\frac{s_{t}+s_{t-1}}{2}&gt;\\frac{1}{n\\times\\lambda}\\)\n¬†¬†¬†¬†¬†Where \\(\\frac{(s_t+s_{t-1})}{2}\\) is the product‚Äôs average share between the two periods\n¬†¬†¬†¬†¬†\\(n\\) ‚Äì the number of products\n¬†¬†¬†¬†¬†\\(\\lambda\\) ‚Äì 1.25\n¬†¬†¬†¬†¬†For example, if \\(n\\) = 80 then items with an average expenditure share greater than 1% are selected\n\nStep 3: If an item is missing ‚Äì impute its price once by multiplying the last observed price by the Jevons of the category movement (i.e.¬†‚Äòclass mean imputation‚Äô method), i.e.¬†the output of step 5 below. This is needed as a strict matched-item method will exclude temporary observed items from the computation.\nStep 4: If explicit adjustments needed such as package changes ‚Äì can be made manually:\n\nExamples are change in package sizes that are really the same for the consumer (i.e.¬†EANs are basically too detailed, and we need a way to link/group 2 products together).\n\nStep 5: Aggregate the price relatives in sample for this and the previous month using a Jevons (unweighted) index method at the elementary level. The month-to-month Jevons is chained to obtain a long-term time series.\nStep 6: Integrate retailer specific COICOP6 indices together using annual chained Laspeyres method (i.e.¬†higher-level aggregation). Scanner data can be used to weigh different retailers (when present) but sub-COICOP6 weight may be needed to separate out non-scanner data sources that use a different aggregation method."
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html#categories-applicable",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html#categories-applicable",
    "title": "Sampling methods for scanner data",
    "section": "Categories applicable",
    "text": "Categories applicable\nThe Dutch case showcased the application of the method for several COICOP categories, specifically Food (01), Wine and Beer (0212, 0213), Tools, household maintenance tools (055, 056), medical and pharmaceutical products (061), pet food (0934), and personal care products (1313). This showcases the application of the method."
  },
  {
    "objectID": "content/blogs/price-stats/sampling-scanner-data/index.html#further-reading-1",
    "href": "content/blogs/price-stats/sampling-scanner-data/index.html#further-reading-1",
    "title": "Sampling methods for scanner data",
    "section": "Further reading",
    "text": "Further reading\nFor more info, check out Grient and Haan (2010) and Grient and Haan (2011)."
  },
  {
    "objectID": "content/blogs/tech-topics/index.html",
    "href": "content/blogs/tech-topics/index.html",
    "title": "Technical topics",
    "section": "",
    "text": "Benchmark test in file formats\n\n\n\ndata engineering\n\nfile format benchmarks\n\n\n\n\n\n\nOct 13, 2020\n\n\n\n\n\n\n\nExplore python itables library to render tables\n\n\n\ndata visualization\n\nquarto\n\n\n\n\n\n\nOct 7, 2025\n\n\n\n\n\n\n\nMultilingual Quarto site\n\n\n\nquarto\n\n\n\n\n\n\nJan 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/blogs/datasets/index.html",
    "href": "content/blogs/datasets/index.html",
    "title": "Datasets",
    "section": "",
    "text": "Explore Italian web scraped grocery dataset\n\n\n\nweb scraped data\n\n\n\n\n\n\nSep 14, 2025\n\n\n\n\n\n\n\nNew Zealand scanner electronics dataset\n\n\n\nscanner data\n\n\n\n\n\n\nOct 3, 2025\n\n\n\n\n\n\n\nPolish scanner dataset\n\n\n\nscanner data\n\n\n\n\n\n\nFeb 2, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "About me",
    "section": "",
    "text": "Hi! üëã I am Serge! Welcome to my site!\nI am a senior data scientist and an independent researcher. While my focus tends to be on price statistics and Machine Learning, I have also written on computational social science and other topics.\nI keep this site to summarize my own thoughts and empirical experiments. All content shared here is based on public information and is strictly my opinion (usually based on a set of sources that I try to reference).\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/cv.html",
    "href": "content/cv.html",
    "title": "CV",
    "section": "",
    "text": "Work experience‚Ä¶\n\n\n\n Back to top"
  },
  {
    "objectID": "content/blogs/index.html",
    "href": "content/blogs/index.html",
    "title": "Blog",
    "section": "",
    "text": "Series\n\n\n\nPrice Statistics\nThis series is on topics in such as price indices and price measurement.\n\n\n\n\n\nDatasets\nThis series exploes various open datasets.\n\n\n\n\n\nTechnical topics\nThis series explores various technical topics, such as about tools, libraries, and frameworks.\n\n\n\n\n\nAll blog posts\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nPolish scanner dataset\n\n\n\nscanner data\n\n\n\n\n\n\nFeb 2, 2026\n\n\n\n\n\n\n\nMultilingual Quarto site\n\n\n\nquarto\n\n\n\n\n\n\nJan 3, 2026\n\n\n\n\n\n\n\nExplore python itables library to render tables\n\n\n\ndata visualization\n\nquarto\n\n\n\n\n\n\nOct 7, 2025\n\n\n\n\n\n\n\nNew Zealand scanner electronics dataset\n\n\n\nscanner data\n\n\n\n\n\n\nOct 3, 2025\n\n\n\n\n\n\n\nExplore Italian web scraped grocery dataset\n\n\n\nweb scraped data\n\n\n\n\n\n\nSep 14, 2025\n\n\n\n\n\n\n\nSampling methods for scanner data\n\n\n\nscanner data\n\nsampling\n\n\n\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\nNotes on bilateral price index methods\n\n\n\nBilateral price indices\n\n\n\n\n\n\nSep 27, 2024\n\n\n\n\n\n\n\nBenchmark test in file formats\n\n\n\ndata engineering\n\nfile format benchmarks\n\n\n\n\n\n\nOct 13, 2020\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/blogs/price-stats/index.html",
    "href": "content/blogs/price-stats/index.html",
    "title": "Price statistics series",
    "section": "",
    "text": "Notes on bilateral price index methods\n\n\n\nBilateral price indices\n\n\n\n\n\n\nSep 27, 2024\n\n\n\n\n\n\n\nSampling methods for scanner data\n\n\n\nscanner data\n\nsampling\n\n\n\n\n\n\nNov 1, 2024\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html",
    "href": "content/blogs/price-stats/bilateral-indices/index.html",
    "title": "Notes on bilateral price index methods",
    "section": "",
    "text": "A price index provides an aggregate measure of price change for a particular product segment, industry, or overall economy. ‚Ä¶ [They compare] the cost of purchasing a set of goods at different points in time. This ‚Äúset of goods‚Äù is often referred to as the ‚Äúmarket basket‚Äù or the ‚Äúbundle‚Äù of goods. - Aizcorbe (2014)\n\n\n\n\nTo illustrate how each index can be calculated, I‚Äôll illustrate using some example datasets and some common price index methods.\n\n\n\n\n\n\nNoteSetup with gpindex\n\n\n\n\n\ngpindex is a convenient R package for price index calculation. It has a simple play dataset built in that we can use to demonstrate how to construct various price indices.\n\nlibrary(gpindex)\n\nWarning: package 'gpindex' was built under R version 4.5.2\n\np1 &lt;- price6$t1\np2 &lt;- price6$t2\nq1 &lt;- quantity6$t1\nq2 &lt;- quantity6$t2\n\nhead(price6)\n\n  t1  t2  t3  t4  t5\n1  1 1.2 1.0 0.8 1.0\n2  1 3.0 1.0 0.5 1.0\n3  1 1.3 1.5 1.6 1.6\n4  1 0.7 0.5 0.3 0.1\n5  1 1.4 1.7 1.9 2.0\n6  1 0.8 0.6 0.4 0.2\n\nhead(quantity6)\n\n   t1  t2  t3  t4   t5\n1 1.0 0.8 1.0 1.2  0.9\n2 1.0 0.9 1.1 1.2  1.2\n3 2.0 1.9 1.8 1.9  2.0\n4 1.0 1.3 3.0 6.0 12.0\n5 4.5 4.7 5.0 5.6  6.5\n6 0.5 0.6 0.8 1.3  2.5"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html#the-concept-of-price-index-methods",
    "href": "content/blogs/price-stats/bilateral-indices/index.html#the-concept-of-price-index-methods",
    "title": "Notes on bilateral price index methods",
    "section": "",
    "text": "A price index provides an aggregate measure of price change for a particular product segment, industry, or overall economy. ‚Ä¶ [They compare] the cost of purchasing a set of goods at different points in time. This ‚Äúset of goods‚Äù is often referred to as the ‚Äúmarket basket‚Äù or the ‚Äúbundle‚Äù of goods. - Aizcorbe (2014)"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html#example-datasets",
    "href": "content/blogs/price-stats/bilateral-indices/index.html#example-datasets",
    "title": "Notes on bilateral price index methods",
    "section": "",
    "text": "To illustrate how each index can be calculated, I‚Äôll illustrate using some example datasets and some common price index methods.\n\n\n\n\n\n\nNoteSetup with gpindex\n\n\n\n\n\ngpindex is a convenient R package for price index calculation. It has a simple play dataset built in that we can use to demonstrate how to construct various price indices.\n\nlibrary(gpindex)\n\nWarning: package 'gpindex' was built under R version 4.5.2\n\np1 &lt;- price6$t1\np2 &lt;- price6$t2\nq1 &lt;- quantity6$t1\nq2 &lt;- quantity6$t2\n\nhead(price6)\n\n  t1  t2  t3  t4  t5\n1  1 1.2 1.0 0.8 1.0\n2  1 3.0 1.0 0.5 1.0\n3  1 1.3 1.5 1.6 1.6\n4  1 0.7 0.5 0.3 0.1\n5  1 1.4 1.7 1.9 2.0\n6  1 0.8 0.6 0.4 0.2\n\nhead(quantity6)\n\n   t1  t2  t3  t4   t5\n1 1.0 0.8 1.0 1.2  0.9\n2 1.0 0.9 1.1 1.2  1.2\n3 2.0 1.9 1.8 1.9  2.0\n4 1.0 1.3 3.0 6.0 12.0\n5 4.5 4.7 5.0 5.6  6.5\n6 0.5 0.6 0.8 1.3  2.5"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html#laspeyres-index",
    "href": "content/blogs/price-stats/bilateral-indices/index.html#laspeyres-index",
    "title": "Notes on bilateral price index methods",
    "section": "Laspeyres index",
    "text": "Laspeyres index\nThe simplest formula (and one that is commonly used) is the Laspeyres index:\n\\[\nI^{L}_{0,1} = \\frac{\\sum_{n=1}^{N}(p_n^1q_n^0)}{\\sum_{n=1}^{N}(p_n^0q_n^0)}\n\\]\nWhere \\(p\\) and \\(q\\) denote prices and quantities, and 0 and 1 denote two points in time.\nIf \\(N\\) goods are sold in both periods (note that an overlap is needed), we can compare the the cost of purchasing the same goods we bought in period 1 with a certain period in the future.\nWe can also write the Laspeyres as the weighted arithmetic average of the price change of the individual products in the index\n\\[\nI^L_{0,1} = \\sum_{n=1}^{N} (w_n^0\\frac{p_n^1}{p_n^0})\n\\]\n\nsuch that\n\n\\[\nw_m^0 = (p_n^0q_n^0)/\\sum_{n=1}^{N}(p_n^0q_n^0)\n\\]\ngive the ratio of good \\(n\\) expenditure to total expenditure, or could also be considered the relative importance or share of the product. There are some key nuances with this approach:\n\nProducts sold in both periods are included in both periods, thus new products are omitted.\nWe fix the relative importance of the goods for both periods based on period 1 weight, thus we do not reflect changes in composition over time (substitution). This can be convenient as we need weights only for the base period.\n\nMost price indices are variants of the Laspeyres, such as the Lowe (which compare the prices from the current month with the previous month, but use weights from a year before that).\n\n\n\n\n\n\n\n\nNoteExample with gpindex\n\n\n\n\n\nThere are two ways that we can use the package to compare the two periods - one using the laspeyres_index() function, the other using the arithmetic_mean() function.\n\nlaspeyres_index(p2, p1, q1)\n\n[1] 1.42\n\n\n\narithmetic_mean(p2 / p1, index_weights(\"Laspeyres\")(p1, q1))\n\n[1] 1.42"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html#paashe-index",
    "href": "content/blogs/price-stats/bilateral-indices/index.html#paashe-index",
    "title": "Notes on bilateral price index methods",
    "section": "Paashe Index",
    "text": "Paashe Index\nSimilar to the Lasperyes, however uses a different basket (the one from the pricing period):\n\\[\nI^P_{0,1} =\n  \\sum_{n=1}^N\n    (\n      p_n^1\n      q_n^1\n    ) /\n  \\sum_{n=1}^N\n    (\n      p_n^0\n      q_n^1\n    )\n\\]\nThe Paasche may also be expressed as a function of the weighted average (i.e.¬†shares):\n\\[\nI^P_{0,1} =\n  1/\n  \\sum_{n=1}^N\n    (\n      w_n^1\n      \\frac\n        {p_n^0}\n        {p_n^1}\n    )\n\\]\n\nsuch that\n\n\\[\nw_{n,1} =\n  (\n    p_n^1\n    q_n^1\n  )/\n  \\sum_{n=1}^n\n  (\n    w_n^1\n    p_n^0\n    q_n^1\n  )\n\\]\nThe Laspeyres and Paasche include prices and quantities in both periods, and both use the same relatives with different expenditure shares.\n\n\n\n\n\n\nNoteExample with gpindex\n\n\n\n\n\nThere are two ways that we can use the package to compare the two periods - one using the paasche_index() function, the other using the arithmetic_mean() function.\n\npaasche_index(p2, p1, q2)\n\n[1] 1.382353\n\n\n\narithmetic_mean(p2 / p1, index_weights(\"Paasche\")(p1, q2))\n\n[1] 1.382353"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html#fisher-index",
    "href": "content/blogs/price-stats/bilateral-indices/index.html#fisher-index",
    "title": "Notes on bilateral price index methods",
    "section": "Fisher Index",
    "text": "Fisher Index\nThe Fisher does a geometric average of the Laspeyres and the Paasche:\n\\[\nI^F_{0,1} = (\n  I^L_{0,1}\n  I^P_{0,1})^\\frac{1}{2}\n\\]\nThe Fisher thus uses expenditure from both periods and thus provides relative importance that are more closely aligned with the goods actually sold. As the Fisher satisfies homogeneity, symmetry, and the time reversal test (the price change from the base to the current should be the inverse of the current to the base) - thus it doesn‚Äôt matter what period is chosen as the base.\n\n\n\n\n\n\nNoteExample with gpindex\n\n\n\n\n\nThere is an out of the box function we can use - the fisher_index() function.\n\nfisher_index(p2, p1, q2, q1)\n\n[1] 1.40105"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html#t√∂rnqvist-index",
    "href": "content/blogs/price-stats/bilateral-indices/index.html#t√∂rnqvist-index",
    "title": "Notes on bilateral price index methods",
    "section": "T√∂rnqvist Index",
    "text": "T√∂rnqvist Index\nSimilar to the Fisher, however it takes the average of the weights instead of averaging the two indices\nIn logged form: \\[\nlnI^T_{0,1}=\n  \\sum^N_{n=1}\n  (\n    w_{n,0}+\n    w_{n,1}\n  )/2\n  (\n    ln\\frac\n      {p_{n,1}}\n      {p_{n,0}}\n  )\n\\]\nWe can also view the T√∂rnqvist as an exponenet of logged form (quite similar to the above):\n\\[\nP^T(p^1,q^1,p^0,q^0) = exp\\{\n  \\sum_{n=1}^N\n    \\frac{1}{2}\n      (s_n^1+s_n^1)\n      ln(p_n^1/p_n^0)\n  \\}\n\\]\nAnd also in more traditional form:\n\\[\nP^T(p^1,q^1,p^0,q^0) =\n\\prod_{n=1}^N\n\\left(\n\\frac{p_n^1}{p_n^0}\n\\right)\n^{\\frac{s_n^1+s_n^0}{2}}\n\\]\n\n\n\n\n\n\nNoteExample with gpindex\n\n\n\n\n\nAs there is no out of the box function for the T√∂rnqvist, we can use the geometric_mean() function.\n\n# 1. Calculate value shares for both periods\ns1 &lt;- (p1 * q1) / sum(p1 * q1)\ns2 &lt;- (p2 * q2) / sum(p2 * q2)\n\n# 2. Calculate T√∂rnqvist weights (arithmetic average of shares)\nw_tornqvist &lt;- (s1 + s2) / 2\n\n# 3. Compute the index using geometric_mean()\ngeometric_mean(p2/p1, w_tornqvist)\n\n[1] 1.405162"
  },
  {
    "objectID": "content/blogs/price-stats/bilateral-indices/index.html#jevons-index",
    "href": "content/blogs/price-stats/bilateral-indices/index.html#jevons-index",
    "title": "Notes on bilateral price index methods",
    "section": "Jevons Index",
    "text": "Jevons Index\nThe Jevons is unweighted geometric mean. Similar to the T√∂rnqvist, in logged form:\n\\[\nlnI^J_{0,1} =\n  \\frac{1}{N}\n  \\sum^N_{n=1}\n    ln(p_n^1/p_n^0)\n\\]\nThe Jevons takes the unweighted average by replacing the \\((w_{m,0}+w_{m,1})/2\\) with \\(1/M\\), thus giving each model equal weight\nThe Jevons can also be written in simpler form as per Balk(2008) (formula 1.5).\n\\[\nP^J(p^1,p^0) =\n\\prod_{n=1}^N\n\\left(\n\\frac{p_n^1}{p_n^0}\n\\right)\n^{1/N}\n\\]\n\n\n\n\n\n\nNoteExample with gpindex\n\n\n\n\n\nThere are two ways that we can use the package to compare the two periods - one using the jevons_index() function, the other using the geometric_mean() function.\n\njevons_index(p2, p1)\n\n[1] 1.24192\n\n\n\ngeometric_mean(p2 / p1)\n\n[1] 1.24192"
  },
  {
    "objectID": "content/blogs/tech-topics/multilingual-quarto/index.html",
    "href": "content/blogs/tech-topics/multilingual-quarto/index.html",
    "title": "Multilingual Quarto site",
    "section": "",
    "text": "‚ÄúThere are no solutions‚Äîthere are only tradeoffs.‚Äù -Thomas Sowell"
  },
  {
    "objectID": "content/blogs/tech-topics/multilingual-quarto/index.html#option-1-quarto-profiles",
    "href": "content/blogs/tech-topics/multilingual-quarto/index.html#option-1-quarto-profiles",
    "title": "Multilingual Quarto site",
    "section": "Option 1: Quarto profiles",
    "text": "Option 1: Quarto profiles\nQuarto profiles is at a glance an effective option as it leverages Quarto functionality (which was technically intended for something else). Instead (or alongside) using profiles for environments, you set up language specific profiles and create a toggle to switch between them. Your project will leverage one _quarto.yml, meaning that you have one standard project with shared configuration. This may help in some nuanced ways, such creating one folder structure for your project (which helps simplify navigation), helps slightly simplify developing common components (as you could with some limitation use one qmd file for both languages for ex, or make writing code once (i.e.¬†do not repeat yourself approach) a tad simpler.2\n\nOverview\n\nSetup a _quarto-{language}.yml for each language profile and language specific metadata (such as navigation) alongside the a regular _quarto.yml for overall project setup (and for shared content).\nRender each specific language profile separately (i.e.¬†quarto render --profile en renders the _quarto_en.yml to _site/en) as well as a shared content that connects the two sub-sites (such as central index.qmd , 404.qmd, etc).\nWhen it comes to writing content, there are some options:\n\nFor simple cases, one .qmd file can be used with language specific sections (using conditional content feature, i.e.¬†{.content-visible when-profile=\"en\"} can contain content in that language.\n\nPros - quite easy to start with!\nCons\n\nI could see this approach being simple at first but get really messy quickly.\nYou have to get creative in setting metadata in other languages, and you face a low bar for metadata you can use. For example you if you want your page title to be in a language of your choice - you must use _quarto-{language}.yml to specify the title and can‚Äôt use the title: bar in the page. If you want to specify advanced metadata like description or tags, you have to do this yourself manually.3\n\n\nFor more complex scenarios (such as to keep cleaner qmd files or to add multilingual metadata), you can separate language specific pages (i.e.¬†*.en.qmd and *.fr.qmd). For content that needs to be shared between both languages, a separate file can be created and integrated, such as using {{ &lt; include _shared-content.qmd &gt;}}.\n\nPros: can start separating content for cleanliness\nCons: this makes the directory really messy as you basically double each file. In this case its almost easier to go to Option 2.\n\n\nA toggle between languages can be hard coded to redirect to the home page of the other language or handled dynamically (a custom metadata tag can be added to each .qmd file and referenced in the _quarto-{language}.yml (i.e.¬†href: ../fr/content/blogs/tech-topics/multilingual-quarto/index.html will reference filename: \"/blog/blog1\" in each qmd file).\n\n\n\nChallenges\n\nIf translation of all page metadata is needed, it becomes necessary to specify the info using language specific qmd files (similar to option 3 as babelquarto requires the same).\nI didn‚Äôt explore this, but image that if you want to use Quarto profiles for its other intended purpose (such as environments), then it could get more complex quite quickly as you‚Äôd be dealing with 4 or more files.\n\n\n\nNotable examples / resources\n\n\n\n\n\n\nImportantPersonal examples\n\n\n\nThis site is organized using this approach (see the repo). In setting it up, I also created a separate vanilla example: https://github.com/sergegoussev/multilingual-quarto-ex\n\n\nI would also recommend:\n\nMario Angst‚Äôs blog provides a good intro to the topic"
  },
  {
    "objectID": "content/blogs/tech-topics/multilingual-quarto/index.html#option-2-separate-quarto-projects-in-one-repo",
    "href": "content/blogs/tech-topics/multilingual-quarto/index.html#option-2-separate-quarto-projects-in-one-repo",
    "title": "Multilingual Quarto site",
    "section": "Option 2: Separate Quarto projects in one repo",
    "text": "Option 2: Separate Quarto projects in one repo\nThe second option for even more customization is to setup separate directories per language with separate _quarto.yml per language. This setup creates two (or more) isolated quarto projects. This is quite similar to using profiles with separate language directories, but ensures more isolation between language.\n\nOverview\n\nUsing separate projects ensures that you have maximum isolation between each language part of the site. However to not have to repeat yourself (such as to not have to duplicate css, code, or other content), you can put them in common folders.\nEach project is rendered separately and combined in a custom way\n\n\n\n\n\n\n\nCautionArea still WIP\n\n\n\nI‚Äôll prob explore this option in a bit more detail and flush out the description.\n\n\n/root\n  ‚îú‚îÄ‚îÄ _common/\n  ‚îÇ    ‚îú‚îÄ‚îÄ index.qmd    &lt;-- home landing page\n  ‚îÇ    ‚îú‚îÄ‚îÄ 404.qmd \n  ‚îÇ    ‚îú‚îÄ‚îÄ styles.css\n  ‚îÇ    ‚îî‚îÄ‚îÄ theme.scss\n  ‚îú‚îÄ‚îÄ en/\n  ‚îÇ    ‚îú‚îÄ‚îÄ _quarto.yml  &lt;-- references ../_common/theme.scss\n  ‚îÇ    ‚îî‚îÄ‚îÄ index.qmd\n  ‚îî‚îÄ‚îÄ fr/\n       ‚îú‚îÄ‚îÄ _quarto.yml  &lt;-- references ../_common/theme.scss\n       ‚îî‚îÄ‚îÄ index.qmd\n\n\nExamples / further reading\n\n\n\n\n\n\nImportantPersonal example\n\n\n\nIn exploring this option, I set up this example: https://github.com/sergegoussev/multilingual-separate-projects-example/\n\n\nA few examples seem to be:\n\nGuillem Maya‚Äôs site seems to leverage this approach as the repo follows the structure."
  },
  {
    "objectID": "content/blogs/tech-topics/multilingual-quarto/index.html#option-3-babelquarto",
    "href": "content/blogs/tech-topics/multilingual-quarto/index.html#option-3-babelquarto",
    "title": "Multilingual Quarto site",
    "section": "Option 3: babelquarto",
    "text": "Option 3: babelquarto\nbabelquarto is a third option. It requires the use of R (instead of quarto) and is not rendered from CRAN but from R Open Science. The option is very similar to option 1 if separate language specific files are used.\n\nOverview\n\nInstead of doing the customization of language switching and being careful about rendering and redirects, babelquarto can do the setup for you. As its an R package, you need to use R to render locally and within your publish.yml to have GitHub render your site. Once set up, it basically makes it easier to maintain language switching as you don‚Äôt have to do this part yourself and let babelquarto do it for you.\n\n\n\nNuances\n\nThe biggest issue that I can see is that the main language is rendered from the root directory, whereas the other languages use a sub-directory. In other words if English is the default, its rendered from project/index.html and French as a secondary would be rendered from project/fr/index.html. This is counter to i18n guidelines as consistency between languages is recommended.\n\nThis also makes it a tad less intuitive if you need to setup a default index.html page to give visitors a choice about which site they wish to proceed to (as with the other two options).\n\nAs a separate R package, this option comes with its own nuances and tradeoffs, including:\n\nUsing it introduces a new dependency. You have to learn how to use it and keep an eye on its evolution and interaction with Quarto.\nBy learning and maintaining your skills in babelquarto, you are adding a to your knowledge of Quarto in a way that does not make you more competent in Quarto itself. In other words, this option slightly broadens your cognitive load as you need to also know this package and still know quarto, whereas deeper expertise in just quarto may serve you well in the future.\n\nAs you are setting up and running R to render you need to ensure your setup is appropriate. For instance a GitHub runner needs to takes a bit longer to setup R and install your renv.lock and also render compared to just using Quarto. T\n\nFurthermore, walking away from Quarto CLI features means you are loosing some ability to customize things that the CLI gives you, such as rendering a specific language to a specific directory (say quarto render ‚Äîprofile: en ‚Äì )\n\n\n\n\nNotable examples / resources\n\n\n\n\n\n\nImportantPersonal examples\n\n\n\nIn exploring this option, I created a separate vanilla example: https://github.com/sergegoussev/babelquarto-example.\n\n\nIf this sounds interesting, have a look at:\n\nA blog by Joel Nitta ‚ÄúMultilingual Webpages with `Babelquarto` (Part 1 of 2).‚Äù¬†2024. December 6, 2024\n\nHe also provides a separate vanilla example (less complex than the one I set up): https://github.com/joelnitta/example-babelquarto/\n\nbabelquarto home site"
  },
  {
    "objectID": "content/blogs/tech-topics/multilingual-quarto/index.html#footnotes",
    "href": "content/blogs/tech-topics/multilingual-quarto/index.html#footnotes",
    "title": "Multilingual Quarto site",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is a rich discussion on the topic here: https://github.com/quarto-dev/quarto-cli/issues/275‚Ü©Ô∏é\nA few like Martin Angst wrote about it.‚Ü©Ô∏é\nCheck out the thread on multilingual Quarto for examples of this.‚Ü©Ô∏é\nSee example by Lino Galiana.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html",
    "href": "content/blogs/datasets/nz-electronics/index.html",
    "title": "New Zealand scanner electronics dataset",
    "section": "",
    "text": "In 2019, Frances Krsinich and Donal Lynch of StatsNZ (working with Harpal Shergill of UNSD) published a synthetic scanner dataset that could be used for various types of research topics, including on multilateral and for quality adjustment methods. The dataset is hosted on the UN Global Platform GitLab and is provided for researchers.\nThis short blog is a short exploration of the dataset to better understand it."
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#original-dataset",
    "href": "content/blogs/datasets/nz-electronics/index.html#original-dataset",
    "title": "New Zealand scanner electronics dataset",
    "section": "Original dataset",
    "text": "Original dataset\nThe data is tabular and it shows products sold per period with a large number of characteristics already pre-cleaned\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5509 entries, 0 to 5508\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   month_num   5509 non-null   object \n 1   char11      5509 non-null   object \n 2   char1       5509 non-null   float64\n 3   char2       5509 non-null   int64  \n 4   char3       5509 non-null   object \n 5   char4       5509 non-null   object \n 6   char5       5509 non-null   object \n 7   char6       5509 non-null   object \n 8   char7       5509 non-null   object \n 9   char8       5509 non-null   object \n 10  char9       5509 non-null   object \n 11  char10      5509 non-null   object \n 12  prodid_num  5509 non-null   int64  \n 13  quantity    5509 non-null   int64  \n 14  value       5509 non-null   int64  \ndtypes: float64(1), int64(4), object(10)\nmemory usage: 645.7+ KB\n\n\nThe key question is how to interpret all these feature columns and what the overall information is in the data\n\n\nShow the code\nstats = {}\nstats['Number of unique products (prodid_num column)'] = df['prodid_num'].nunique()\nstats['Number of months in sample'] = df['month_num'].nunique()\nstats['First month in sample'] = df['month_num'].min()\nstats['Last month in sample'] = df['month_num'].max()\nstats['Char11 unique count (brands)'] = df['char11'].nunique()\nstats['Char1 unique count (possibly screen size)'] = df['char1'].nunique()\nstats['Char10 unique count'] = df['char10'].nunique()\n\npd.DataFrame.from_dict(stats, orient='index', columns=['statistic'])\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#overview-of-the-issue",
    "href": "content/blogs/datasets/nz-electronics/index.html#overview-of-the-issue",
    "title": "New Zealand scanner electronics dataset",
    "section": "Overview of the issue",
    "text": "Overview of the issue\nEvery period has products that seem to be duplicated - i.e.¬†products are identical in all but sale information. We can see this by comparing the number of unique products (if we use the prodid_num column) and comparing it with just a count of the same products (without de-duplication).\n\n\nShow the code\nseries_unique = df.groupby(['month_num'])['prodid_num'].nunique()\nseries_count = df.groupby(['month_num'])['prodid_num'].count()\n\ndf2 = pd.DataFrame({'Unique': series_unique, 'Count': series_count}).reset_index()\n\nfig = px.line(df2, x='month_num', y=['Unique', 'Count'], \n              title='Number of unique products per period and number of total products per period',\n              labels={'month_num': 'Time period', 'value': 'Count of the number of products'})\n\nfig.update_layout(yaxis_title='Number of products', xaxis_title='Month')\nfig.show()"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#examples",
    "href": "content/blogs/datasets/nz-electronics/index.html#examples",
    "title": "New Zealand scanner electronics dataset",
    "section": "Examples",
    "text": "Examples\nLooking at a few examples - it is clear that all product features are identical except the quanity and the value counts\n\n\nShow the code\ndf_first_period = df[df['month_num'] == df['month_num'].min()].copy()\ndf_first_period.groupby(['prodid_num']).filter(lambda x: x['char11'].count() &gt; 1).head(2)\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nLets look at another example\n\n\nShow the code\ndf_first_period.groupby(['prodid_num']).filter(lambda x: x['char11'].count() &gt; 1).iloc[2:4]\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nAnd another\n\n\nShow the code\ndf_first_period.groupby(['prodid_num']).filter(lambda x: x['char11'].count() &gt; 1).iloc[4:6]\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nAs the dataset was created by taking a subset of variables of real data and modifying it, its possible there were variables that differentiated these products that were not included in the synthetic dataset (prodid_num was generated after the modification process).\nTo make use of the data then, it will be necessary to aggregate these duplicate products by summing the quantity and value columns when calculating unit prices. This is an assumption but it is a workable one."
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#fixing-the-issue-and-creating-a-clean-dataset",
    "href": "content/blogs/datasets/nz-electronics/index.html#fixing-the-issue-and-creating-a-clean-dataset",
    "title": "New Zealand scanner electronics dataset",
    "section": "Fixing the issue and creating a clean dataset",
    "text": "Fixing the issue and creating a clean dataset\nTo simplify downstream analysis, and upload this simpified one to Zenodo - fix the issue and create a new version of the dataste (i.e.¬†do a group by in a way that keeps the other key info).\n\n# Define the aggregation dictionary (i.e. logic of the groupby) - the idea is to \n# ensure that unique product id is kept, with scanner info summed.\nagg_dict = {\n    'char11': 'first',\n    'char1':'first',\n    'char2':'first',\n    'char3':'first',\n    'char4':'first',\n    'char5':'first',\n    'char6':'first',\n    'char7':'first',\n    'char8':'first',\n    'char9':'first',\n    'char10':'first',\n    'quantity':'sum',\n    'value':'sum'\n}\n\n# fix the uniqueness issue using \ndf_aggregated = df.groupby(['month_num','prodid_num']).agg(agg_dict)\ndf_aggregated.reset_index(inplace=True)\n# Create a unit price column\ndf_aggregated['unit_price'] = df_aggregated['value']/df_aggregated['quantity']\ndf_aggregated.to_csv(\"../data/gold/NZ_dataset_historic_aggregated_secure.csv\")"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#overall-info",
    "href": "content/blogs/datasets/nz-electronics/index.html#overall-info",
    "title": "New Zealand scanner electronics dataset",
    "section": "Overall info",
    "text": "Overall info\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3843 entries, 0 to 3842\nData columns (total 16 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   month_num   3843 non-null   object \n 1   prodid_num  3843 non-null   int64  \n 2   char11      3843 non-null   object \n 3   char1       3843 non-null   float64\n 4   char2       3843 non-null   int64  \n 5   char3       3843 non-null   object \n 6   char4       3843 non-null   object \n 7   char5       3843 non-null   object \n 8   char6       3843 non-null   object \n 9   char7       3843 non-null   object \n 10  char8       3843 non-null   object \n 11  char9       3843 non-null   object \n 12  char10      3843 non-null   object \n 13  quantity    3843 non-null   int64  \n 14  value       3843 non-null   int64  \n 15  unit_price  3843 non-null   float64\ndtypes: float64(2), int64(4), object(10)\nmemory usage: 480.5+ KB\n\n\n\n\nShow the code\nstats = {}\nstats['Number of unique products (prodid_num column)'] = df_aggregated['prodid_num'].nunique()\nstats['Number of months in sample'] = df_aggregated['month_num'].nunique()\nstats['First month in sample'] = df_aggregated['month_num'].min()\nstats['Last month in sample'] = df_aggregated['month_num'].max()\nstats['Char11 unique count (brands)'] = df_aggregated['char11'].nunique()\nstats['Char1 unique count (possibly screen size)'] = df_aggregated['char1'].nunique()\nstats['Char10 unique count'] = df_aggregated['char10'].nunique()\n\npd.DataFrame.from_dict(stats, orient='index', columns=['statistic'])\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#tracking-number-of-unique-products-over-time",
    "href": "content/blogs/datasets/nz-electronics/index.html#tracking-number-of-unique-products-over-time",
    "title": "New Zealand scanner electronics dataset",
    "section": "Tracking number of unique products over time",
    "text": "Tracking number of unique products over time\n\n\nShow the code\nimport plotly.graph_objects as go\n\nseries_count = df_aggregated.groupby(['month_num'])['prodid_num'].count().to_frame().reset_index()\nseries_count['Average'] = series_count['prodid_num'].mean()\n\nfig = px.bar(series_count, x='month_num', y='prodid_num', title='Number of unique products per month')\n\n# add moving-average line\nfig.add_trace(\n    go.Scatter(\n        x=series_count['month_num'],\n        y=series_count['Average'],\n        mode='lines',\n        name='Average',\n        line=dict(color='red', width=1.5 , dash='dash')\n    )\n)\n\nfig.update_layout(xaxis_title='Month', yaxis_title='Count')\nfig.show()"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#average-prices-across-time",
    "href": "content/blogs/datasets/nz-electronics/index.html#average-prices-across-time",
    "title": "New Zealand scanner electronics dataset",
    "section": "Average prices across time",
    "text": "Average prices across time\n\n\nShow the code\nprice_trend = df_aggregated.groupby([\"month_num\"])[\"unit_price\"].mean().reset_index()\n\nfig2 = px.line(price_trend, x='month_num', y='unit_price', title='Average price per month', range_y=[0, None])\nfig2.update_layout(xaxis_title='Month', yaxis_title='Price (in NZD)')\nfig2.show()"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#churn-in-products",
    "href": "content/blogs/datasets/nz-electronics/index.html#churn-in-products",
    "title": "New Zealand scanner electronics dataset",
    "section": "Churn in products",
    "text": "Churn in products\n\n\nShow the code\ndf_aggregated2 = df_aggregated.reset_index()\nn = round(df_aggregated2.groupby(['prodid_num'])['month_num'].count().mean(),1)\n\n\"The average length of time products are in sample: {n}\".format(n=n)\n\n\n'The average length of time products are in sample: 8.4'\n\n\n\nProduct lifetimes (Gantt-like)\nWe can represent visually the longevity of products in sample using a gantt -like view where a line represents a product‚Äôs time in the sample based on when it entered and existed."
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html",
    "href": "content/blogs/tech-topics/file-types/index.html",
    "title": "Benchmark test in file formats",
    "section": "",
    "text": "When doing data analysis with big data, scaling is often a concern as the files we are working with are large. Hence we want to select file formats that are appropriate - have low on-disk usage and having fast input-output (i.e.¬†read-write). This workbook does a benchmark assessment of a few well known file types. It is quite similar to other benchmark studies, such as this ‚Äòtowards data science‚Äô format study by Ilia Zaitsev in 2017."
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#file-formats-analyzed",
    "href": "content/blogs/tech-topics/file-types/index.html#file-formats-analyzed",
    "title": "Benchmark test in file formats",
    "section": "File formats analyzed",
    "text": "File formats analyzed\nOld school file formats: 1. Pain CSV 2. Excel (xlsx)\nApache Arrow formats: 3. Parquet 4. Feather\nPython specific formats: 5. Pickle 6. Compressed pickle (using zip format)\nOther data formats: 7. HDF5"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#variables-used-in-the-analysis",
    "href": "content/blogs/tech-topics/file-types/index.html#variables-used-in-the-analysis",
    "title": "Benchmark test in file formats",
    "section": "Variables used in the analysis",
    "text": "Variables used in the analysis\n\ntime to save a file (output to stored memory);\ntime to load a file (input from stored memory);\nspace on disk"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#dataset-utilized-in-analyis",
    "href": "content/blogs/tech-topics/file-types/index.html#dataset-utilized-in-analyis",
    "title": "Benchmark test in file formats",
    "section": "Dataset utilized in analyis",
    "text": "Dataset utilized in analyis\nTo analyze each file format, the relatively large dataset from Dominiks‚Äô Fine foods scanner dataset was utilized as it contains a range of variables - numeric, string, and integer ‚Äì as well as being large (7.3M rows)."
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#note",
    "href": "content/blogs/tech-topics/file-types/index.html#note",
    "title": "Benchmark test in file formats",
    "section": "Note",
    "text": "Note\nAs a side comment, while this analysis was done on Python, the conclusions are mostly applicable to R or other langauges exclusing the use of Pickle and compressed pickle formats, which are python formats.\n\n# %matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\nSetup a simple timer to return time taken for a task\n\nimport datetime\n\nclass Timer:\n    \"\"\"\n    Simple timer. When first initiated, it starts, has one method stop(), it prints the time taken\n    \n    `\n    t = Timer()\n    t.stop()\n    `\n    \n    By default, the Timer() will just print the length of time taken, however if you specify\n    The stop(return_time=True), it will instead return a datetime object of time taken \n    \"\"\"\n    def __init__(self):\n        # when first initiated, start the clock\n        self.t_start = datetime.datetime.now()\n        \n    def stop(self, return_time=False):\n        # end timer\n        self.t_end = datetime.datetime.now()\n        # return or print the length of time taken\n        if return_time == True:\n            return self.t_end - self.t_start\n        else:\n            print(\"Task took {t}\".format(t=self.t_end - self.t_start))\n\nLoad the dataset used in the demo and see how long it is\nNOTE: at this step, any .csv demo dataset can be substituted\n\ndemo_dataset = \"https://www.chicagobooth.edu/-/media/enterprise/centers/kilts/datasets/dominicks-dataset/movement_csv-files/wana_csv.zip\"\n\ndf = pd.read_csv(demo_dataset)\nlen(df)\n\n7339217\n\n\nConsidering that there are 7.3M rows of data, this invalidates xlsx as the simple (or full) output type as this can only handle 1 million rows. If we had still wanted to work with excel for such a large dataset, we would have to split it into several 1M row files. Hence to simulate excel, we will just focus on saving or reading one 1M row file. To equalize the analysis at the end of the day, we will simply multiply the time taken to load/save/store 1m rows by 7.3\n\nexcel_muliplier = len(df)/1000000"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#setup-process-to-automate-the-tests",
    "href": "content/blogs/tech-topics/file-types/index.html#setup-process-to-automate-the-tests",
    "title": "Benchmark test in file formats",
    "section": "Setup Process to automate the tests",
    "text": "Setup Process to automate the tests\n\ndef execute_command(message, command):\n    print(\"starting test:\",message)\n    total = None\n    for each in range(0,4):\n        t = Timer()\n        exec(command)\n        if total == None:\n            total = t.stop(return_time=True)\n        else:\n            total = total + t.stop(return_time=True)\n    print(\"took {l}\".format(l=total/5))\n    time_av = total/5\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if message[:4] == \"xlsx\":\n        time_av = time_av * excel_muliplier\n    return message, time_av.total_seconds()\n\n\nformats = {\n    \"pickle - write\":\"df.to_pickle('{}'.format(file_paths['pickle']))\",\n    \"pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['pickle']))\",\n    \"compressed pickle - write\":\"df.to_pickle('{}'.format(file_paths['compressed pickle']), compression='zip')\",\n    \"compressed pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['compressed pickle']))\",\n    \"csv - write\":\"df.to_csv('{}'.format(file_paths['csv']))\",\n    \"csv - read\":\"df = pd.read_csv('{}'.format(file_paths['csv']))\",\n    \"parquet - write\":\"df.to_parquet('{}'.format(file_paths['parquet']))\",\n    \"parquet - read\":\"df = pd.read_parquet('{}'.format(file_paths['parquet']))\",\n    \"feather - write\":\"df.to_feather('{}'.format(file_paths['feather']))\",\n    \"feather - read\":\"df = pd.read_feather('{}'.format(file_paths['feather']))\",\n    \"hdf5 - write\":\"df.to_hdf('{}'.format(file_paths['hdf5']), key='df')\",\n    \"hdf5 - read\":\"df = pd.read_hdf('{}'.format(file_paths['hdf5']))\",\n    \"xlsx - write\":\"df[:1000000].to_excel('{}'.format(file_paths['xlsx']))\",\n    \"xlsx - read\":\"df = pd.read_excel('{}'.format(file_paths['xlsx']))\"\n}\n\nfile_paths = {\n    \"csv\":\"wana.csv\",\n    \"pickle\":\"wana.pkl\",\n    \"compressed pickle\":\"wana.pkl.zip\",\n    \"feather\":\"wana.feather\",\n    \"parquet\":\"wana.parquet.gzip\",\n    \"hdf5\":\"wana.h5\",\n    \"xlsx\":\"wana.xlsx\"\n}"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#run-the-process-on-the-downloaded-demo-file",
    "href": "content/blogs/tech-topics/file-types/index.html#run-the-process-on-the-downloaded-demo-file",
    "title": "Benchmark test in file formats",
    "section": "Run the process on the downloaded demo file",
    "text": "Run the process on the downloaded demo file\n\ntimes = []\nfor _format in formats:\n    times.append(execute_command(_format, formats[_format]))\n\ndf_processing_test = pd.DataFrame(times, columns=[\"task\", \"time taken\"])\n\nstarting test: pickle - write\ntook 0:00:03.344310\nstarting test: pickle - read\ntook 0:00:01.997607\nstarting test: compressed pickle - write\ntook 0:00:07.953267\nstarting test: compressed pickle - read\ntook 0:00:12.442646\nstarting test: csv - write\ntook 0:00:40.201943\nstarting test: csv - read\ntook 0:00:06.481310\nstarting test: parquet - write\ntook 0:00:04.330219\nstarting test: parquet - read\ntook 0:00:03.246392\nstarting test: feather - write\ntook 0:00:02.894822\nstarting test: feather - read\ntook 0:00:01.323537\nstarting test: hdf5 - write\n\n\nC:\\Users\\gouss\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py:2449: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type-&gt;mixed,key-&gt;block2_values] [items-&gt;Index(['SALE', 'PRICE_HEX', 'PROFIT_HEX'], dtype='object')]\n\n  encoding=encoding,\n\n\ntook 0:00:03.991776\nstarting test: hdf5 - read\ntook 0:00:05.148564\nstarting test: xlsx - write\ntook 0:02:35.857042\nstarting test: xlsx - read\ntook 0:01:36.240834\n\n\n\nfile_size_test = []\n\nfor file in file_paths:\n    fsize = os.path.getsize(file_paths[file])\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if file[:4] == \"xlsx\":\n        fsize = fsize * excel_muliplier\n    file_size_test.append((file, fsize/1024/1024))\n\ndf_file_size_test = pd.DataFrame(file_size_test, columns=['type','file size (MB)'])"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#visualize-and-print-results-for-analysis",
    "href": "content/blogs/tech-topics/file-types/index.html#visualize-and-print-results-for-analysis",
    "title": "Benchmark test in file formats",
    "section": "Visualize and print results for analysis",
    "text": "Visualize and print results for analysis\nNow that all the results are availible, lets visualize them a bit prior to qualitative analysis\n\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet'),\n Text(0, 0, 'hdf5'),\n Text(0, 0, 'xlsx')]\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read'),\n Text(0, 0, 'hdf5 - write'),\n Text(0, 0, 'hdf5 - read'),\n Text(0, 0, 'xlsx - write'),\n Text(0, 0, 'xlsx - read')]\n\n\n\n\n\n\n\n\n\nOkay‚Ä¶ so hdf5 and xlsx are clearly not favourable. If we drop them, what does the result look like\n\ndf_file_size_test_without_outliers = df_file_size_test.drop([5,6])\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test_without_outliers)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved\\n(dropped hdf5 as outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet')]\n\n\n\n\n\n\n\n\n\n\ndf_processing_test_without_outliers = df_processing_test.drop([10,11,12,13])\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test_without_outliers)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO\\n(dropped excel as extreme outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read')]"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#detailed-results",
    "href": "content/blogs/tech-topics/file-types/index.html#detailed-results",
    "title": "Benchmark test in file formats",
    "section": "Detailed results",
    "text": "Detailed results\nWe can print out the dataframes of interst to see the detailed data for all the files, including the outliers\n\ndf_file_size_test\n\n\n\n\n\n\n\n\ntype\nfile size (MB)\n\n\n\n\n0\ncsv\n547.119179\n\n\n1\npickle\n579.979185\n\n\n2\ncompressed pickle\n14.979172\n\n\n3\nfeather\n104.476992\n\n\n4\nparquet\n20.218006\n\n\n5\nhdf5\n2088.185272\n\n\n6\nxlsx\n288.665235\n\n\n\n\n\n\n\n\ndf_processing_test\n\n\n\n\n\n\n\n\ntask\ntime taken\n\n\n\n\n0\npickle - write\n3.344310\n\n\n1\npickle - read\n1.997607\n\n\n2\ncompressed pickle - write\n7.953267\n\n\n3\ncompressed pickle - read\n12.442646\n\n\n4\ncsv - write\n40.201943\n\n\n5\ncsv - read\n6.481310\n\n\n6\nparquet - write\n4.330219\n\n\n7\nparquet - read\n3.246392\n\n\n8\nfeather - write\n2.894822\n\n\n9\nfeather - read\n1.323537\n\n\n10\nhdf5 - write\n3.991776\n\n\n11\nhdf5 - read\n5.148564\n\n\n12\nxlsx - write\n1143.868652\n\n\n13\nxlsx - read\n706.332365"
  }
]