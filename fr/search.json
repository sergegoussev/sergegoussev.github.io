[
  {
    "objectID": "content/blogs/tech-topics/itables/index.html",
    "href": "content/blogs/tech-topics/itables/index.html",
    "title": "Explore python itables library to render tables",
    "section": "",
    "text": "As much of the data I use is tabular - I often have to visualize the dataframe, pivot it, filter it, and then visualize it again. Since I may want to go back to the dataset later - making sure I can repeat this mini-EDA again - is quite useful. Downloading an excel for manual exploration is obviously an option but not an ideal one.\nPython has an itables library that helps render dataframes as pretty tables. These can also be integrated with quarto to render into the html.\nThis blog summarizes several key features and how to turn them on.\n\nimport pandas as pd\n\nfrom itables import init_notebook_mode\nfrom itables import show\n\nimport itables.options as opt\n\n#raise the size of the data made availible othewize medium (or even some small) dataframes will be downsampled.\nopt.maxBytes = 131072\nopt.maxColumns = 0\n\ninit_notebook_mode(all_interactive=True)\n\nopt.ordering = False \n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#purpose-of-the-blog",
    "href": "content/blogs/tech-topics/itables/index.html#purpose-of-the-blog",
    "title": "Explore python itables library to render tables",
    "section": "",
    "text": "As much of the data I use is tabular - I often have to visualize the dataframe, pivot it, filter it, and then visualize it again. Since I may want to go back to the dataset later - making sure I can repeat this mini-EDA again - is quite useful. Downloading an excel for manual exploration is obviously an option but not an ideal one.\nPython has an itables library that helps render dataframes as pretty tables. These can also be integrated with quarto to render into the html.\nThis blog summarizes several key features and how to turn them on.\n\nimport pandas as pd\n\nfrom itables import init_notebook_mode\nfrom itables import show\n\nimport itables.options as opt\n\n#raise the size of the data made availible othewize medium (or even some small) dataframes will be downsampled.\nopt.maxBytes = 131072\nopt.maxColumns = 0\n\ninit_notebook_mode(all_interactive=True)\n\nopt.ordering = False \n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#example-data",
    "href": "content/blogs/tech-topics/itables/index.html#example-data",
    "title": "Explore python itables library to render tables",
    "section": "Example data",
    "text": "Example data\nLets generate a complex set of data that has a diverse set of columns\n\ndata_for_df = {\n    'product_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115],\n    'price': [12.50, 19.99, 5.00, 25.75, 45.20, 10.99, 8.50, 30.00, 9.99, 15.25, 20.00, 7.50, 22.40, 18.99, 35.10],\n    'category': ['Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries', 'Electronics', 'Books', 'Groceries'],\n    'customer_rating': [4, 5, 3, 5, 4, 5, 4, 3, 5, 4, 5, 3, 4, 5, 4],\n    'manufacturer': [\n        '&lt;p&gt;&lt;b&gt;Advanced Technology Solutions, a global leader&lt;/b&gt;&lt;p&gt;',\n        'Independent Authors Collective, specializing in unique fiction',\n        'Small-Town Organic Farmers Cooperative',\n        'Global Tech Innovations Group',\n        'The Classic Book Reprint Company, preserving literary heritage',\n        'Fresh Harvest Supply Chain and Distribution',\n        'Audio Excellence, an innovative headphones manufacturer',\n        'The Aspiring Coder\\'s Press and Digital Media',\n        'Sustainable Coffee Roasters Association',\n        'Smart Living Devices and Automation Solutions',\n        'Renowned Historical Accounts Publishing House',\n        'Local Produce Delivery for the Modern Consumer',\n        'Elite Computing Innovations, a leader in high-performance hardware',\n        'Emerging Voices Poetry Guild',\n        'Professional Imaging and Optics Corporation'\n    ],\n    'region': ['North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia', 'North America', 'Europe', 'Asia'],\n    'long_description': [\n        'This is a long description for the first product, highlighting its premium features and durability.',\n        'An enthralling novel with a captivating plot and unforgettable characters that will keep you on the edge of your seat.',\n        'Freshly sourced organic apples, perfect for a healthy snack or for baking delicious homemade pies.',\n        'A high-performance gadget with advanced technology and a sleek, modern design that is easy to use and carry.',\n        'A classic mystery story with an unexpected twist, a great read for all fans of the genre.',\n        'A selection of artisanal breads, baked fresh daily with high-quality ingredients and a crispy crust.',\n        'Innovative noise-canceling headphones providing crystal-clear audio and a comfortable fit for extended listening sessions.',\n        'A comprehensive guide to coding best practices, offering practical examples and detailed explanations for beginners.',\n        'Gourmet coffee beans sourced from sustainable farms, roasted to perfection for a rich and smooth flavor.',\n        'A versatile smart home device that can control various appliances and is compatible with most other systems.',\n        'A riveting biography of a historical figure, full of intriguing details and little-known anecdotes from their life.',\n        'Hand-picked seasonal vegetables delivered fresh to your door, perfect for preparing a wide variety of meals.',\n        'An ultra-thin tablet with a high-resolution display, making it ideal for both entertainment and professional use.',\n        'A timeless poetry collection featuring works from both well-known and up-and-coming authors.',\n        'A powerful and compact camera perfect for capturing high-quality photos and videos on the go.'\n    ]\n}\n\ndf = pd.DataFrame.from_dict(data_for_df)\n\nNow lets see some ways this can be worked with"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#default-visual",
    "href": "content/blogs/tech-topics/itables/index.html#default-visual",
    "title": "Explore python itables library to render tables",
    "section": "Default visual",
    "text": "Default visual\n\nshow(df)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/tech-topics/itables/index.html#advanced-version",
    "href": "content/blogs/tech-topics/itables/index.html#advanced-version",
    "title": "Explore python itables library to render tables",
    "section": "Advanced version",
    "text": "Advanced version\nLets make some changes so that we can modify column widths, add word-wrap, and change defaults for specific columns\n\nshow(\n    df,\n    # Disable auto-width and use a fixed layout for reliable column width.\n    autoWidth=False,\n    style=\"table-layout:fixed\",#; width:800px;\",\n    # Add buttons to customie actions that can be taken\n    buttons=['pageLength', 'csvHtml15','colvis'],\n    # Add a search pagen to ensure that I can filter the data in some way\n    layout={\"top1\":'searchPanes'},\n    # Modify columns\n    columnDefs=[\n        # Modify width of overly wide columns and make them word wrap\n        {\"width\": \"200px\", \"targets\": [4, 6], \"className\": \"word-wrap\"},\n        # Hide some columns by default\n        {\"targets\": [2,3], \"visible\": False},\n    ],\n    scrollX=False,\n)\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html",
    "href": "content/blogs/datasets/nz-electronics/index.html",
    "title": "New Zealand scanner electronics dataset",
    "section": "",
    "text": "In 2019, Frances Krsinich and Donal Lynch of StatsNZ (working with Harpal Shergill of UNSD) published a synthetic scanner dataset that could be used for various types of research topics, including on multilateral and for quality adjustment methods. The dataset is hosted on the UN Global Platform GitLab and is provided for researchers.\nThis short blog is a short exploration of the dataset to better understand it."
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#original-dataset",
    "href": "content/blogs/datasets/nz-electronics/index.html#original-dataset",
    "title": "New Zealand scanner electronics dataset",
    "section": "Original dataset",
    "text": "Original dataset\nThe data is tabular and it shows products sold per period with a large number of characteristics already pre-cleaned\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5509 entries, 0 to 5508\nData columns (total 15 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   month_num   5509 non-null   object \n 1   char11      5509 non-null   object \n 2   char1       5509 non-null   float64\n 3   char2       5509 non-null   int64  \n 4   char3       5509 non-null   object \n 5   char4       5509 non-null   object \n 6   char5       5509 non-null   object \n 7   char6       5509 non-null   object \n 8   char7       5509 non-null   object \n 9   char8       5509 non-null   object \n 10  char9       5509 non-null   object \n 11  char10      5509 non-null   object \n 12  prodid_num  5509 non-null   int64  \n 13  quantity    5509 non-null   int64  \n 14  value       5509 non-null   int64  \ndtypes: float64(1), int64(4), object(10)\nmemory usage: 645.7+ KB\n\n\nThe key question is how to interpret all these feature columns and what the overall information is in the data\n\n\nShow the code\nstats = {}\nstats['Number of unique products (prodid_num column)'] = df['prodid_num'].nunique()\nstats['Number of months in sample'] = df['month_num'].nunique()\nstats['First month in sample'] = df['month_num'].min()\nstats['Last month in sample'] = df['month_num'].max()\nstats['Char11 unique count (brands)'] = df['char11'].nunique()\nstats['Char1 unique count (possibly screen size)'] = df['char1'].nunique()\nstats['Char10 unique count'] = df['char10'].nunique()\n\npd.DataFrame.from_dict(stats, orient='index', columns=['statistic'])\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#overview-of-the-issue",
    "href": "content/blogs/datasets/nz-electronics/index.html#overview-of-the-issue",
    "title": "New Zealand scanner electronics dataset",
    "section": "Overview of the issue",
    "text": "Overview of the issue\nEvery period has products that seem to be duplicated - i.e.Â products are identical in all but sale information. We can see this by comparing the number of unique products (if we use the prodid_num column) and comparing it with just a count of the same products (without de-duplication).\n\n\nShow the code\nseries_unique = df.groupby(['month_num'])['prodid_num'].nunique()\nseries_count = df.groupby(['month_num'])['prodid_num'].count()\n\ndf2 = pd.DataFrame({'Unique': series_unique, 'Count': series_count}).reset_index()\n\nfig = px.line(df2, x='month_num', y=['Unique', 'Count'], \n              title='Number of unique products per period and number of total products per period',\n              labels={'month_num': 'Time period', 'value': 'Count of the number of products'})\n\nfig.update_layout(yaxis_title='Number of products', xaxis_title='Month')\nfig.show()"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#examples",
    "href": "content/blogs/datasets/nz-electronics/index.html#examples",
    "title": "New Zealand scanner electronics dataset",
    "section": "Examples",
    "text": "Examples\nLooking at a few examples - it is clear that all product features are identical except the quanity and the value counts\n\n\nShow the code\ndf_first_period = df[df['month_num'] == df['month_num'].min()].copy()\ndf_first_period.groupby(['prodid_num']).filter(lambda x: x['char11'].count() &gt; 1).head(2)\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nLets look at another example\n\n\nShow the code\ndf_first_period.groupby(['prodid_num']).filter(lambda x: x['char11'].count() &gt; 1).iloc[2:4]\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nAnd another\n\n\nShow the code\ndf_first_period.groupby(['prodid_num']).filter(lambda x: x['char11'].count() &gt; 1).iloc[4:6]\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nAs the dataset was created by taking a subset of variables of real data and modifying it, its possible there were variables that differentiated these products that were not included in the synthetic dataset (prodid_num was generated after the modification process).\nTo make use of the data then, it will be necessary to aggregate these duplicate products by summing the quantity and value columns when calculating unit prices. This is an assumption but it is a workable one."
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#fixing-the-issue-and-creating-a-clean-dataset",
    "href": "content/blogs/datasets/nz-electronics/index.html#fixing-the-issue-and-creating-a-clean-dataset",
    "title": "New Zealand scanner electronics dataset",
    "section": "Fixing the issue and creating a clean dataset",
    "text": "Fixing the issue and creating a clean dataset\nTo simplify downstream analysis, and upload this simpified one to Zenodo - fix the issue and create a new version of the dataste (i.e.Â do a group by in a way that keeps the other key info).\n\n# Define the aggregation dictionary (i.e. logic of the groupby) - the idea is to \n# ensure that unique product id is kept, with scanner info summed.\nagg_dict = {\n    'char11': 'first',\n    'char1':'first',\n    'char2':'first',\n    'char3':'first',\n    'char4':'first',\n    'char5':'first',\n    'char6':'first',\n    'char7':'first',\n    'char8':'first',\n    'char9':'first',\n    'char10':'first',\n    'quantity':'sum',\n    'value':'sum'\n}\n\n# fix the uniqueness issue using \ndf_aggregated = df.groupby(['month_num','prodid_num']).agg(agg_dict)\ndf_aggregated.reset_index(inplace=True)\n# Create a unit price column\ndf_aggregated['unit_price'] = df_aggregated['value']/df_aggregated['quantity']\ndf_aggregated.to_csv(\"../data/gold/NZ_dataset_historic_aggregated_secure.csv\")"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#overall-info",
    "href": "content/blogs/datasets/nz-electronics/index.html#overall-info",
    "title": "New Zealand scanner electronics dataset",
    "section": "Overall info",
    "text": "Overall info\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3843 entries, 0 to 3842\nData columns (total 16 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   month_num   3843 non-null   object \n 1   prodid_num  3843 non-null   int64  \n 2   char11      3843 non-null   object \n 3   char1       3843 non-null   float64\n 4   char2       3843 non-null   int64  \n 5   char3       3843 non-null   object \n 6   char4       3843 non-null   object \n 7   char5       3843 non-null   object \n 8   char6       3843 non-null   object \n 9   char7       3843 non-null   object \n 10  char8       3843 non-null   object \n 11  char9       3843 non-null   object \n 12  char10      3843 non-null   object \n 13  quantity    3843 non-null   int64  \n 14  value       3843 non-null   int64  \n 15  unit_price  3843 non-null   float64\ndtypes: float64(2), int64(4), object(10)\nmemory usage: 480.5+ KB\n\n\n\n\nShow the code\nstats = {}\nstats['Number of unique products (prodid_num column)'] = df_aggregated['prodid_num'].nunique()\nstats['Number of months in sample'] = df_aggregated['month_num'].nunique()\nstats['First month in sample'] = df_aggregated['month_num'].min()\nstats['Last month in sample'] = df_aggregated['month_num'].max()\nstats['Char11 unique count (brands)'] = df_aggregated['char11'].nunique()\nstats['Char1 unique count (possibly screen size)'] = df_aggregated['char1'].nunique()\nstats['Char10 unique count'] = df_aggregated['char10'].nunique()\n\npd.DataFrame.from_dict(stats, orient='index', columns=['statistic'])\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#tracking-number-of-unique-products-over-time",
    "href": "content/blogs/datasets/nz-electronics/index.html#tracking-number-of-unique-products-over-time",
    "title": "New Zealand scanner electronics dataset",
    "section": "Tracking number of unique products over time",
    "text": "Tracking number of unique products over time\n\n\nShow the code\nimport plotly.graph_objects as go\n\nseries_count = df_aggregated.groupby(['month_num'])['prodid_num'].count().to_frame().reset_index()\nseries_count['Average'] = series_count['prodid_num'].mean()\n\nfig = px.bar(series_count, x='month_num', y='prodid_num', title='Number of unique products per month')\n\n# add moving-average line\nfig.add_trace(\n    go.Scatter(\n        x=series_count['month_num'],\n        y=series_count['Average'],\n        mode='lines',\n        name='Average',\n        line=dict(color='red', width=1.5 , dash='dash')\n    )\n)\n\nfig.update_layout(xaxis_title='Month', yaxis_title='Count')\nfig.show()"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#average-prices-across-time",
    "href": "content/blogs/datasets/nz-electronics/index.html#average-prices-across-time",
    "title": "New Zealand scanner electronics dataset",
    "section": "Average prices across time",
    "text": "Average prices across time\n\n\nShow the code\nprice_trend = df_aggregated.groupby([\"month_num\"])[\"unit_price\"].mean().reset_index()\n\nfig2 = px.line(price_trend, x='month_num', y='unit_price', title='Average price per month', range_y=[0, None])\nfig2.update_layout(xaxis_title='Month', yaxis_title='Price (in NZD)')\nfig2.show()"
  },
  {
    "objectID": "content/blogs/datasets/nz-electronics/index.html#churn-in-products",
    "href": "content/blogs/datasets/nz-electronics/index.html#churn-in-products",
    "title": "New Zealand scanner electronics dataset",
    "section": "Churn in products",
    "text": "Churn in products\n\n\nShow the code\ndf_aggregated2 = df_aggregated.reset_index()\nn = round(df_aggregated2.groupby(['prodid_num'])['month_num'].count().mean(),1)\n\n\"The average length of time products are in sample: {n}\".format(n=n)\n\n\n'The average length of time products are in sample: 8.4'\n\n\n\nProduct lifetimes (Gantt-like)\nWe can represent visually the longevity of products in sample using a gantt -like view where a line represents a productâ€™s time in the sample based on when it entered and existed."
  },
  {
    "objectID": "content/blogs/tech-topics/multilingual-quarto/index.html",
    "href": "content/blogs/tech-topics/multilingual-quarto/index.html",
    "title": "Quarto multilingue",
    "section": "",
    "text": "ImportantCe blog nâ€™est pas encore traduit\n\n\n\nCliquez ici pour le lire en anglais.\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "content/blogs/price-stats/index.html",
    "href": "content/blogs/price-stats/index.html",
    "title": "SÃ©rie de statistiques sur les prix",
    "section": "",
    "text": "Aucun article correspondant\n Retour au sommet"
  },
  {
    "objectID": "content/blogs/index.html",
    "href": "content/blogs/index.html",
    "title": "Blog",
    "section": "",
    "text": "Trier par\n      Ordre par dÃ©faut\n      \n        Date - Le plus ancien\n      \n      \n        Date - Le plus rÃ©cent\n      \n      \n        Titre\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitre\n\n\n\nCatÃ©gories\n\n\n\nDate\n\n\n\n\n\n\n\n\nBenchmark test in file formats\n\n\ndata engineering, file format benchmarks\n\n\n13 oct. 2020\n\n\n\n\n\n\nExplore Italian web scraped grocery dataset\n\n\nweb scraped data\n\n\n14 sept. 2025\n\n\n\n\n\n\nExplore python itables library to render tables\n\n\ndata visualization, quarto\n\n\n7 oct. 2025\n\n\n\n\n\n\nNew Zealand scanner electronics dataset\n\n\nscanner data\n\n\n3 oct. 2025\n\n\n\n\n\n\nQuarto multilingue\n\n\nQuartos\n\n\n3 janv. 2026\n\n\n\n\n\n\nAucun article correspondant\n Retour au sommet"
  },
  {
    "objectID": "content/cv.html",
    "href": "content/cv.html",
    "title": "CV",
    "section": "",
    "text": "ExpÃ©rience de travailâ€¦\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Ã€ propos de moi",
    "section": "",
    "text": "Salut ! ðŸ‘‹ Je suis Serge ! Bienvenue sur mon site !\nJe suis scientifique des donnÃ©es principal et chercheur indÃ©pendant. Bien que je me spÃ©cialise dans les statistiques de prix et lâ€™apprentissage machine, jâ€™ai aussi Ã©crit sur les sciences sociales computationnelles et dâ€™autres sujets.\nCe site me permet de synthÃ©tiser mes rÃ©flexions et mes expÃ©riences empiriques. Tout le contenu partagÃ© ici est basÃ© sur des informations publiques et reflÃ¨te uniquement mon opinion (gÃ©nÃ©ralement fondÃ©e sur un ensemble de sources que je mâ€™efforce de citer).\n\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "content/blogs/datasets/index.html",
    "href": "content/blogs/datasets/index.html",
    "title": "Les ensembles de donnÃ©es",
    "section": "",
    "text": "Explore Italian web scraped grocery dataset\n\n\n\nweb scraped data\n\n\n\n\n\n\n14 sept. 2025\n\n\n\n\n\n\n\nNew Zealand scanner electronics dataset\n\n\n\nscanner data\n\n\n\n\n\n\n3 oct. 2025\n\n\n\n\n\nAucun article correspondant\n Retour au sommet"
  },
  {
    "objectID": "content/blogs/tech-topics/index.html",
    "href": "content/blogs/tech-topics/index.html",
    "title": "Sujets techniques",
    "section": "",
    "text": "Benchmark test in file formats\n\n\n\ndata engineering\n\nfile format benchmarks\n\n\n\n\n\n\n13 oct. 2020\n\n\n\n\n\n\n\nExplore python itables library to render tables\n\n\n\ndata visualization\n\nquarto\n\n\n\n\n\n\n7 oct. 2025\n\n\n\n\n\n\n\nQuarto multilingue\n\n\n\nQuartos\n\n\n\n\n\n\n3 janv. 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAucun article correspondant\n Retour au sommet"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html",
    "href": "content/blogs/datasets/italian-ws/index.html",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "",
    "text": "Daniele Sasso and a few others made their dataset availible on Zenodo - https://doi.org/10.5281/zenodo.14927602 - daily webscraped data from different shops of an Italian supermarket chain. This blog summarizes the dataset and explores its various facets. Detailed overview of the data is available on the Price Stats Catalogue record of this dataset and some explorations below are summarized there."
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#dataset-structure",
    "href": "content/blogs/datasets/italian-ws/index.html#dataset-structure",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Dataset structure",
    "text": "Dataset structure\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#general-overview-of-the-dataset",
    "href": "content/blogs/datasets/italian-ws/index.html#general-overview-of-the-dataset",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "General overview of the dataset",
    "text": "General overview of the dataset\nFirst off - lets look at the data itself, its columns, and some statistics about the web scraping itself.\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4033211 entries, 0 to 4033210\nData columns (total 8 columns):\n #   Column      Dtype  \n---  ------      -----  \n 0   date        object \n 1   price       float64\n 2   product_id  int64  \n 3   store_id    int64  \n 4   region      object \n 5   product     object \n 6   COICOP5     object \n 7   COICOP4     object \ndtypes: float64(1), int64(2), object(5)\nmemory usage: 246.2+ MB"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#detailed-info-about-the-dataset",
    "href": "content/blogs/datasets/italian-ws/index.html#detailed-info-about-the-dataset",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Detailed info about the dataset",
    "text": "Detailed info about the dataset\nAs this is web scrape data for several years - its saved all in one analytical table.\nLetâ€™s look at it in a bit more detail:\n\n\nShow the code\nstats = {}\nstats['Number of unique products'] = df['product'].nunique()\nstats['Number of unique stores'] = df['store_id'].nunique()\nstats['Number of unique regions'] = df['region'].nunique()\nstats['Number of COICOP5 categories'] = df['COICOP5'].nunique()\nstats['Number of unique scrapes'] = df['date'].nunique()\nstats['Number of average unique products per store per date'] = round(df.groupby([\"date\", \"store_id\"])[\"product_id\"].nunique().reset_index()['product_id'].mean(),1)\nd_end = datetime.fromisoformat(df['date'].max())\nd_start = datetime.fromisoformat(df['date'].min())\nd = d_end-d_start\nstats['number of days in sample'] = d.days + 1\npd.DataFrame.from_dict(stats, orient='index', columns=['statistic'])\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nIt seems that there are 863 days but 841 scrapes - that means that there were no scrapes during 22 days:\n\n\nShow the code\n# Compare the current scrape list\nscrape_dates = pd.DatetimeIndex(df['date'].unique())\n\n# Against an uninterupted list of dates\nstart_date = scrape_dates.min()\nend_date = scrape_dates.max()\n\n# Create a complete, continuous date range\nfull_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n# Find the dates that are in the full range but not in the scrape list\nmissing_dates = full_date_range.difference(scrape_dates)\n# missing_dates\n\nmissing_dates\n\n\nDatetimeIndex(['2021-02-13', '2021-02-14', '2021-03-23', '2021-03-28',\n               '2021-06-24', '2021-08-22', '2021-08-23', '2021-08-24',\n               '2021-08-25', '2021-08-26', '2021-08-27', '2021-09-30',\n               '2021-12-02', '2022-01-23', '2022-03-16', '2022-06-21',\n               '2022-10-01', '2022-10-07', '2022-10-10', '2022-10-22',\n               '2022-10-23', '2022-11-04'],\n              dtype='datetime64[ns]', freq=None)\n\n\nIf we pivot the raw data and show the number of prices captured per store per region - it looks like this:\n\n\nShow the code\ndf.pivot_table(index='date', columns=['region','store_id'], aggfunc='count')\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\nLets also look at the number of stores per region (i.e.Â the above but visually)\n\n\nShow the code\ndf_number_of_stores_per_region = df.groupby([\"region\"])[\"store_id\"].nunique().to_frame()\ndf_number_of_stores_per_region.plot(kind='bar', color='green', figsize=(10,4))\nplt.title('Number of stores per region')\nplt.xlabel('region')\nplt.ylabel('Number of stores')\n# plt.xticks(rotation=1)  # Keep x-axis labels horizontal\nplt.show()"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#trends-about-what-was-captured",
    "href": "content/blogs/datasets/italian-ws/index.html#trends-about-what-was-captured",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Trends about what was captured",
    "text": "Trends about what was captured\n\nBy category\nLets look at the number of unique products and the number of web offers captured by COICOP5 category\n\n\nShow the code\ndf_coicop_categories = df.groupby([\"COICOP5\"])[\"product_id\"].nunique()\nmean = df_coicop_categories.mean()\n\nfix, ax = plt.subplots()\n\ndf_coicop_categories.plot(\n    kind=\"bar\",\n    figsize=(12,4),\n    title=\"Number of unique products per COICOP5 category\",\n    color='darkblue',\n    legend=False,\n    ax=ax\n)\n\nax.axhline(mean, color='red', alpha=0.5)\n\nax.text(\n    x=19,\n    y=mean,\n    s=f'Average: {round(mean,1)}',\n    color='red',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndf_coicop_categories = df.groupby([\"COICOP5\"])[\"product_id\"].count()\nmean = df_coicop_categories.mean()\n\nfix, ax = plt.subplots()\n\ndf_coicop_categories.plot(\n    kind=\"bar\",\n    figsize=(12,4),\n    title=\"Number of web offers per COICOP5 category\",\n    color=\"#24dba4\",\n    legend=False,\n    ax=ax\n)\n\nax.axhline(mean, color='red', alpha=0.5)\n\nax.text(\n    x=19,\n    y=mean,\n    s=f'Average: {round(mean,1)}',\n    color='red',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nOver time\nWe can also consider how much data was captured across time\n\n\nShow the code\ndf2 = df.copy(deep=True)\ndf2['date'] = pd.to_datetime(df2['date'])\ndf2 = df2.set_index('date')\ndf_scrapes = df2.resample('ME')['product_id'].count().to_frame()\nscrape_mean_monthly = df_scrapes['product_id'].mean()\n\nfix, ax = plt.subplots()\n\ndf_scrapes.plot(\n    kind='bar',\n    figsize=(12,4),\n    title=\"Number of web offers scraped per month, along with an average\",\n    legend=False,\n    ax=ax)\nplt.ylabel(\"number of web offers\")\n\nax.axhline(scrape_mean_monthly, color='red', alpha=0.5, label=\"average\")\n\nax.text(\n    x=20,\n    y=scrape_mean_monthly,\n    s=f'Average: {round(scrape_mean_monthly,1)}',\n    color='red',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nIt seems that the amount of web offers started to decline. This should probably be investigated (if its region or store coverage) to see if longitudinal time series should exclude any of this data"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#geographic-distribution-of-unique-products-by-region",
    "href": "content/blogs/datasets/italian-ws/index.html#geographic-distribution-of-unique-products-by-region",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Geographic distribution of unique products by region",
    "text": "Geographic distribution of unique products by region\nThere is some example code in the zenodo page for the dataset that shows well some of the price/product info captured\n\n\nShow the code\n# Convert date column\ndf['date'] = pd.to_datetime(df['date'])  # Format: YYYY-MM-DD\n\n# Define category colors\ncategory_colors = {\"Fruit\": \"blue\", \"Vegetable\": \"green\", \"Meat\": \"red\"}\n\ngeo = df.groupby([\"region\", \"COICOP4\"])[\"product_id\"].nunique().reset_index()\npivot_geo = geo.pivot(index=\"region\", columns=\"COICOP4\", values=\"product_id\").fillna(0)\npivot_geo[\"Total\"] = pivot_geo.sum(axis=1)\npivot_geo = pivot_geo.sort_values(\"Total\", ascending=False).drop(columns=\"Total\")\npivot_geo = pivot_geo[[\"Fruit\", \"Meat\", \"Vegetable\"]]\n\npivot_geo.plot(kind=\"bar\", stacked=True, figsize=(10,6), color=[\"blue\", \"red\", \"green\"])\nplt.ylabel(\"Number of Unique Products\")\nplt.title(\"Geographic Distribution by Region and Category (Sorted)\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Category\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "content/blogs/datasets/italian-ws/index.html#basic-analysis-average-price-trend-over-time-by-coicop4",
    "href": "content/blogs/datasets/italian-ws/index.html#basic-analysis-average-price-trend-over-time-by-coicop4",
    "title": "Explore Italian web scraped grocery dataset",
    "section": "Basic analysis: average price trend over time (by COICOP4)",
    "text": "Basic analysis: average price trend over time (by COICOP4)\nWe can also look at average prices by COICOP4 over time\n\n\nShow the code\nprice_trend = df.groupby([\"date\", \"COICOP4\"])[\"price\"].mean().reset_index()\n\nplt.figure(figsize=(10,5))\nsns.lineplot(data=price_trend, x=\"date\", y=\"price\", hue=\"COICOP4\", palette=category_colors)\nplt.title(\"Average Price Over Time by COICOP4 Category\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Average Price (â‚¬)\")\nplt.legend(title=\"Category\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html",
    "href": "content/blogs/tech-topics/file-types/index.html",
    "title": "Benchmark test in file formats",
    "section": "",
    "text": "When doing data analysis with big data, scaling is often a concern as the files we are working with are large. Hence we want to select file formats that are appropriate - have low on-disk usage and having fast input-output (i.e.Â read-write). This workbook does a benchmark assessment of a few well known file types. It is quite similar to other benchmark studies, such as this â€˜towards data scienceâ€™ format study by Ilia Zaitsev in 2017."
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#file-formats-analyzed",
    "href": "content/blogs/tech-topics/file-types/index.html#file-formats-analyzed",
    "title": "Benchmark test in file formats",
    "section": "File formats analyzed",
    "text": "File formats analyzed\nOld school file formats: 1. Pain CSV 2. Excel (xlsx)\nApache Arrow formats: 3. Parquet 4. Feather\nPython specific formats: 5. Pickle 6. Compressed pickle (using zip format)\nOther data formats: 7. HDF5"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#variables-used-in-the-analysis",
    "href": "content/blogs/tech-topics/file-types/index.html#variables-used-in-the-analysis",
    "title": "Benchmark test in file formats",
    "section": "Variables used in the analysis",
    "text": "Variables used in the analysis\n\ntime to save a file (output to stored memory);\ntime to load a file (input from stored memory);\nspace on disk"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#dataset-utilized-in-analyis",
    "href": "content/blogs/tech-topics/file-types/index.html#dataset-utilized-in-analyis",
    "title": "Benchmark test in file formats",
    "section": "Dataset utilized in analyis",
    "text": "Dataset utilized in analyis\nTo analyze each file format, the relatively large dataset from Dominiksâ€™ Fine foods scanner dataset was utilized as it contains a range of variables - numeric, string, and integer â€“ as well as being large (7.3M rows)."
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#note",
    "href": "content/blogs/tech-topics/file-types/index.html#note",
    "title": "Benchmark test in file formats",
    "section": "Note",
    "text": "Note\nAs a side comment, while this analysis was done on Python, the conclusions are mostly applicable to R or other langauges exclusing the use of Pickle and compressed pickle formats, which are python formats.\n\n# %matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\nSetup a simple timer to return time taken for a task\n\nimport datetime\n\nclass Timer:\n    \"\"\"\n    Simple timer. When first initiated, it starts, has one method stop(), it prints the time taken\n    \n    `\n    t = Timer()\n    t.stop()\n    `\n    \n    By default, the Timer() will just print the length of time taken, however if you specify\n    The stop(return_time=True), it will instead return a datetime object of time taken \n    \"\"\"\n    def __init__(self):\n        # when first initiated, start the clock\n        self.t_start = datetime.datetime.now()\n        \n    def stop(self, return_time=False):\n        # end timer\n        self.t_end = datetime.datetime.now()\n        # return or print the length of time taken\n        if return_time == True:\n            return self.t_end - self.t_start\n        else:\n            print(\"Task took {t}\".format(t=self.t_end - self.t_start))\n\nLoad the dataset used in the demo and see how long it is\nNOTE: at this step, any .csv demo dataset can be substituted\n\ndemo_dataset = \"https://www.chicagobooth.edu/-/media/enterprise/centers/kilts/datasets/dominicks-dataset/movement_csv-files/wana_csv.zip\"\n\ndf = pd.read_csv(demo_dataset)\nlen(df)\n\n7339217\n\n\nConsidering that there are 7.3M rows of data, this invalidates xlsx as the simple (or full) output type as this can only handle 1 million rows. If we had still wanted to work with excel for such a large dataset, we would have to split it into several 1M row files. Hence to simulate excel, we will just focus on saving or reading one 1M row file. To equalize the analysis at the end of the day, we will simply multiply the time taken to load/save/store 1m rows by 7.3\n\nexcel_muliplier = len(df)/1000000"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#setup-process-to-automate-the-tests",
    "href": "content/blogs/tech-topics/file-types/index.html#setup-process-to-automate-the-tests",
    "title": "Benchmark test in file formats",
    "section": "Setup Process to automate the tests",
    "text": "Setup Process to automate the tests\n\ndef execute_command(message, command):\n    print(\"starting test:\",message)\n    total = None\n    for each in range(0,4):\n        t = Timer()\n        exec(command)\n        if total == None:\n            total = t.stop(return_time=True)\n        else:\n            total = total + t.stop(return_time=True)\n    print(\"took {l}\".format(l=total/5))\n    time_av = total/5\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if message[:4] == \"xlsx\":\n        time_av = time_av * excel_muliplier\n    return message, time_av.total_seconds()\n\n\nformats = {\n    \"pickle - write\":\"df.to_pickle('{}'.format(file_paths['pickle']))\",\n    \"pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['pickle']))\",\n    \"compressed pickle - write\":\"df.to_pickle('{}'.format(file_paths['compressed pickle']), compression='zip')\",\n    \"compressed pickle - read\":\"df = pd.read_pickle('{}'.format(file_paths['compressed pickle']))\",\n    \"csv - write\":\"df.to_csv('{}'.format(file_paths['csv']))\",\n    \"csv - read\":\"df = pd.read_csv('{}'.format(file_paths['csv']))\",\n    \"parquet - write\":\"df.to_parquet('{}'.format(file_paths['parquet']))\",\n    \"parquet - read\":\"df = pd.read_parquet('{}'.format(file_paths['parquet']))\",\n    \"feather - write\":\"df.to_feather('{}'.format(file_paths['feather']))\",\n    \"feather - read\":\"df = pd.read_feather('{}'.format(file_paths['feather']))\",\n    \"hdf5 - write\":\"df.to_hdf('{}'.format(file_paths['hdf5']), key='df')\",\n    \"hdf5 - read\":\"df = pd.read_hdf('{}'.format(file_paths['hdf5']))\",\n    \"xlsx - write\":\"df[:1000000].to_excel('{}'.format(file_paths['xlsx']))\",\n    \"xlsx - read\":\"df = pd.read_excel('{}'.format(file_paths['xlsx']))\"\n}\n\nfile_paths = {\n    \"csv\":\"wana.csv\",\n    \"pickle\":\"wana.pkl\",\n    \"compressed pickle\":\"wana.pkl.zip\",\n    \"feather\":\"wana.feather\",\n    \"parquet\":\"wana.parquet.gzip\",\n    \"hdf5\":\"wana.h5\",\n    \"xlsx\":\"wana.xlsx\"\n}"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#run-the-process-on-the-downloaded-demo-file",
    "href": "content/blogs/tech-topics/file-types/index.html#run-the-process-on-the-downloaded-demo-file",
    "title": "Benchmark test in file formats",
    "section": "Run the process on the downloaded demo file",
    "text": "Run the process on the downloaded demo file\n\ntimes = []\nfor _format in formats:\n    times.append(execute_command(_format, formats[_format]))\n\ndf_processing_test = pd.DataFrame(times, columns=[\"task\", \"time taken\"])\n\nstarting test: pickle - write\ntook 0:00:03.344310\nstarting test: pickle - read\ntook 0:00:01.997607\nstarting test: compressed pickle - write\ntook 0:00:07.953267\nstarting test: compressed pickle - read\ntook 0:00:12.442646\nstarting test: csv - write\ntook 0:00:40.201943\nstarting test: csv - read\ntook 0:00:06.481310\nstarting test: parquet - write\ntook 0:00:04.330219\nstarting test: parquet - read\ntook 0:00:03.246392\nstarting test: feather - write\ntook 0:00:02.894822\nstarting test: feather - read\ntook 0:00:01.323537\nstarting test: hdf5 - write\n\n\nC:\\Users\\gouss\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py:2449: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type-&gt;mixed,key-&gt;block2_values] [items-&gt;Index(['SALE', 'PRICE_HEX', 'PROFIT_HEX'], dtype='object')]\n\n  encoding=encoding,\n\n\ntook 0:00:03.991776\nstarting test: hdf5 - read\ntook 0:00:05.148564\nstarting test: xlsx - write\ntook 0:02:35.857042\nstarting test: xlsx - read\ntook 0:01:36.240834\n\n\n\nfile_size_test = []\n\nfor file in file_paths:\n    fsize = os.path.getsize(file_paths[file])\n    # since excel can only store 1M records, multiply the excel estimate by the datasize ratio\n    if file[:4] == \"xlsx\":\n        fsize = fsize * excel_muliplier\n    file_size_test.append((file, fsize/1024/1024))\n\ndf_file_size_test = pd.DataFrame(file_size_test, columns=['type','file size (MB)'])"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#visualize-and-print-results-for-analysis",
    "href": "content/blogs/tech-topics/file-types/index.html#visualize-and-print-results-for-analysis",
    "title": "Benchmark test in file formats",
    "section": "Visualize and print results for analysis",
    "text": "Visualize and print results for analysis\nNow that all the results are availible, lets visualize them a bit prior to qualitative analysis\n\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet'),\n Text(0, 0, 'hdf5'),\n Text(0, 0, 'xlsx')]\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read'),\n Text(0, 0, 'hdf5 - write'),\n Text(0, 0, 'hdf5 - read'),\n Text(0, 0, 'xlsx - write'),\n Text(0, 0, 'xlsx - read')]\n\n\n\n\n\n\n\n\n\nOkayâ€¦ so hdf5 and xlsx are clearly not favourable. If we drop them, what does the result look like\n\ndf_file_size_test_without_outliers = df_file_size_test.drop([5,6])\nplt.figure(figsize=(15,7))\nax = sns.barplot(x='type', y='file size (MB)', data=df_file_size_test_without_outliers)\n_ = ax.set_xlabel('File Format')\n_ = ax.set_ylabel('Megabytes')\n_ = ax.set_title('File size of the data that is saved\\n(dropped hdf5 as outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large')\n\n[Text(0, 0, 'csv'),\n Text(0, 0, 'pickle'),\n Text(0, 0, 'compressed pickle'),\n Text(0, 0, 'feather'),\n Text(0, 0, 'parquet')]\n\n\n\n\n\n\n\n\n\n\ndf_processing_test_without_outliers = df_processing_test.drop([10,11,12,13])\nplt.figure(figsize=(20,10))\nax = sns.barplot(x='task', y='time taken', data=df_processing_test_without_outliers)\n_ = ax.set_xlabel('File format')\n_ = ax.set_ylabel('Time Taken (seconds)')\n_ = ax.set_title('File formats and the time taken for IO\\n(dropped excel as extreme outlier)')\nax.grid(True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize='x-large', horizontalalignment='right')\n\n[Text(0, 0, 'pickle - write'),\n Text(0, 0, 'pickle - read'),\n Text(0, 0, 'compressed pickle - write'),\n Text(0, 0, 'compressed pickle - read'),\n Text(0, 0, 'csv - write'),\n Text(0, 0, 'csv - read'),\n Text(0, 0, 'parquet - write'),\n Text(0, 0, 'parquet - read'),\n Text(0, 0, 'feather - write'),\n Text(0, 0, 'feather - read')]"
  },
  {
    "objectID": "content/blogs/tech-topics/file-types/index.html#detailed-results",
    "href": "content/blogs/tech-topics/file-types/index.html#detailed-results",
    "title": "Benchmark test in file formats",
    "section": "Detailed results",
    "text": "Detailed results\nWe can print out the dataframes of interst to see the detailed data for all the files, including the outliers\n\ndf_file_size_test\n\n\n\n\n\n\n\n\ntype\nfile size (MB)\n\n\n\n\n0\ncsv\n547.119179\n\n\n1\npickle\n579.979185\n\n\n2\ncompressed pickle\n14.979172\n\n\n3\nfeather\n104.476992\n\n\n4\nparquet\n20.218006\n\n\n5\nhdf5\n2088.185272\n\n\n6\nxlsx\n288.665235\n\n\n\n\n\n\n\n\ndf_processing_test\n\n\n\n\n\n\n\n\ntask\ntime taken\n\n\n\n\n0\npickle - write\n3.344310\n\n\n1\npickle - read\n1.997607\n\n\n2\ncompressed pickle - write\n7.953267\n\n\n3\ncompressed pickle - read\n12.442646\n\n\n4\ncsv - write\n40.201943\n\n\n5\ncsv - read\n6.481310\n\n\n6\nparquet - write\n4.330219\n\n\n7\nparquet - read\n3.246392\n\n\n8\nfeather - write\n2.894822\n\n\n9\nfeather - read\n1.323537\n\n\n10\nhdf5 - write\n3.991776\n\n\n11\nhdf5 - read\n5.148564\n\n\n12\nxlsx - write\n1143.868652\n\n\n13\nxlsx - read\n706.332365"
  }
]